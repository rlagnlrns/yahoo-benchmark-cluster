2020-08-26 09:29:21,055 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-26 09:29:21,067 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-26 09:29:21,074 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-26 09:29:21,459 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-26 09:29:21,596 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-26 09:29:21,596 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-26 09:29:21,599 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-26 09:29:21,600 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-26 09:29:21,839 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-26 09:29:21,904 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-26 09:29:21,914 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-26 09:29:21,934 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-26 09:29:21,941 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-26 09:29:21,944 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-26 09:29:21,944 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-26 09:29:21,944 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-26 09:29:22,097 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-26 09:29:22,098 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-26 09:29:22,115 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-26 09:29:22,115 INFO org.mortbay.log: jetty-6.1.26
2020-08-26 09:29:22,264 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-26 09:29:22,355 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-26 09:29:22,355 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-26 09:29:22,423 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-26 09:29:22,424 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-26 09:29:22,427 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-26 09:29:22,480 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-26 09:29:22,480 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-26 09:29:22,482 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-26 09:29:22,483 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 26 09:29:22
2020-08-26 09:29:22,485 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-26 09:29:22,485 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:29:22,493 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-26 09:29:22,493 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-26 09:29:22,502 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-26 09:29:22,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-26 09:29:22,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-26 09:29:22,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-26 09:29:22,509 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-26 09:29:22,510 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-26 09:29:22,570 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-26 09:29:22,570 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:29:22,570 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-26 09:29:22,571 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-26 09:29:22,571 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-26 09:29:22,571 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-26 09:29:22,572 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-26 09:29:22,572 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-26 09:29:22,580 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-26 09:29:22,580 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:29:22,580 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-26 09:29:22,581 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-26 09:29:22,582 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-26 09:29:22,582 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-26 09:29:22,582 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-26 09:29:22,586 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-26 09:29:22,586 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-26 09:29:22,586 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-26 09:29:22,590 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-26 09:29:22,590 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-26 09:29:22,593 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-26 09:29:22,593 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:29:22,593 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-26 09:29:22,593 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-26 09:29:22,608 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 10178@producer.c.poetic-set-285601.internal
2020-08-26 09:29:22,666 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-26 09:29:22,666 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2020-08-26 09:29:22,666 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-08-26 09:29:22,792 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-08-26 09:29:22,824 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-26 09:29:22,824 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000000
2020-08-26 09:29:22,829 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-26 09:29:22,839 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 10 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-26 09:29:22,840 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-26 09:29:22,840 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2020-08-26 09:29:23,029 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-26 09:29:23,029 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 432 msecs
2020-08-26 09:29:23,187 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-26 09:29:23,194 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-26 09:29:23,210 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-26 09:29:23,285 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-26 09:29:23,298 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-26 09:29:23,298 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-26 09:29:23,298 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-26 09:29:23,299 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-08-26 09:29:23,299 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-26 09:29:23,299 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-26 09:29:23,312 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:29:23,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2020-08-26 09:29:23,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-26 09:29:23,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-08-26 09:29:23,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-26 09:29:23,320 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-26 09:29:23,320 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 21 msec
2020-08-26 09:29:23,353 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-26 09:29:23,354 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-26 09:29:23,367 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-26 09:29:23,367 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-26 09:29:23,382 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-26 09:29:23,792 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-26 09:29:23,792 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:29:23,793 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-26 09:29:23,856 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-26 09:29:23,856 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:29:23,856 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-26 09:29:23,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:29:23,869 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-26 09:29:23,879 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-26 09:29:23,879 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:29:23,879 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-26 09:29:23,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:29:23,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-26 09:29:23,913 INFO BlockStateChange: BLOCK* processReport 0x16271764448b: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 0, hasStaleStorage: false, processing time: 1 msecs
2020-08-26 09:29:23,913 INFO BlockStateChange: BLOCK* processReport 0x1626e0167bfa: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2020-08-26 09:29:23,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:29:23,931 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-26 09:29:23,951 INFO BlockStateChange: BLOCK* processReport 0x1626e0e9547d: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2020-08-26 09:30:28,146 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-26 09:30:28,148 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-26 09:31:06,503 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-26 09:31:06,516 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-26 09:31:06,526 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-26 09:31:06,949 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-26 09:31:07,092 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-26 09:31:07,092 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-26 09:31:07,095 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-26 09:31:07,095 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-26 09:31:07,328 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-26 09:31:07,389 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-26 09:31:07,399 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-26 09:31:07,415 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-26 09:31:07,420 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-26 09:31:07,423 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-26 09:31:07,423 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-26 09:31:07,424 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-26 09:31:07,573 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-26 09:31:07,574 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-26 09:31:07,591 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-26 09:31:07,591 INFO org.mortbay.log: jetty-6.1.26
2020-08-26 09:31:07,755 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-26 09:31:07,787 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-26 09:31:07,787 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-26 09:31:07,831 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-26 09:31:07,832 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-26 09:31:07,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-26 09:31:07,875 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-26 09:31:07,875 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-26 09:31:07,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-26 09:31:07,877 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 26 09:31:07
2020-08-26 09:31:07,883 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-26 09:31:07,883 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:31:07,885 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-26 09:31:07,885 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-26 09:31:07,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-26 09:31:07,892 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-26 09:31:07,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-26 09:31:07,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-26 09:31:07,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-26 09:31:07,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-26 09:31:07,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-26 09:31:07,893 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-26 09:31:07,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-26 09:31:07,900 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-26 09:31:07,901 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-26 09:31:07,901 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-26 09:31:07,902 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-26 09:31:07,968 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-26 09:31:07,968 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:31:07,968 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-26 09:31:07,969 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-26 09:31:07,969 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-26 09:31:07,969 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-26 09:31:07,969 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-26 09:31:07,970 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-26 09:31:07,979 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-26 09:31:07,979 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:31:07,979 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-26 09:31:07,979 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-26 09:31:07,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-26 09:31:07,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-26 09:31:07,981 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-26 09:31:07,985 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-26 09:31:07,985 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-26 09:31:07,985 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-26 09:31:07,989 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-26 09:31:07,989 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-26 09:31:07,991 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-26 09:31:07,991 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-26 09:31:07,991 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-26 09:31:07,991 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-26 09:31:08,008 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 11027@producer.c.poetic-set-285601.internal
2020-08-26 09:31:08,083 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-26 09:31:08,227 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000001 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000001-0000000000000000001
2020-08-26 09:31:08,233 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-08-26 09:31:08,277 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-08-26 09:31:08,303 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-26 09:31:08,303 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000000
2020-08-26 09:31:08,303 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #1
2020-08-26 09:31:08,303 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000001-0000000000000000001
2020-08-26 09:31:08,305 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000001-0000000000000000001' to transaction ID 1
2020-08-26 09:31:08,307 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000001-0000000000000000001 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-26 09:31:08,307 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-26 09:31:08,315 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 8 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-26 09:31:08,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-26 09:31:08,316 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 2
2020-08-26 09:31:08,431 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-26 09:31:08,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 437 msecs
2020-08-26 09:31:08,615 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-26 09:31:08,623 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-26 09:31:08,639 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-26 09:31:08,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-26 09:31:08,735 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-26 09:31:08,736 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-26 09:31:08,736 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-26 09:31:08,736 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-08-26 09:31:08,736 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-26 09:31:08,736 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-26 09:31:08,749 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:31:08,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2020-08-26 09:31:08,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-26 09:31:08,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-08-26 09:31:08,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-26 09:31:08,757 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-26 09:31:08,758 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 20 msec
2020-08-26 09:31:08,787 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-26 09:31:08,788 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-26 09:31:08,792 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-26 09:31:08,792 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-26 09:31:08,807 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-26 09:31:13,315 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-26 09:31:13,315 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:31:13,316 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-26 09:31:13,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-26 09:31:13,325 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:31:13,325 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-26 09:31:13,335 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-26 09:31:13,335 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:31:13,335 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-26 09:31:13,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:31:13,395 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-26 09:31:13,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:31:13,398 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-26 09:31:13,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-26 09:31:13,409 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-26 09:31:13,443 INFO BlockStateChange: BLOCK* processReport 0x164097c0d0e4: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2020-08-26 09:31:13,445 INFO BlockStateChange: BLOCK* processReport 0x16405e3b5eb5: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2020-08-26 09:31:13,453 INFO BlockStateChange: BLOCK* processReport 0x1640604e0d67: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 0, hasStaleStorage: false, processing time: 0 msecs
2020-08-26 09:32:18,074 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-26 09:32:18,074 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-26 09:32:18,074 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 2
2020-08-26 09:32:18,074 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 7 
2020-08-26 09:32:18,076 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 9 
2020-08-26 09:32:18,078 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000002 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000002-0000000000000000003
2020-08-26 09:32:18,078 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 4
2020-08-26 09:32:19,067 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-26 09:32:19,067 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000003 size 326 bytes.
2020-08-26 09:32:19,072 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 0
2020-08-26 09:51:46,209 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2020-08-26 09:52:37,749 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /test/tt.txt._COPYING_
2020-08-26 09:52:38,201 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /test/tt.txt._COPYING_
2020-08-26 09:52:38,201 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-08-26 09:52:38,214 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 15
2020-08-26 09:52:38,215 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741825_1001 size 15
2020-08-26 09:52:38,215 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741825_1001 size 15
2020-08-26 09:52:38,612 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /test/tt.txt._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1419458200_1
2020-08-26 10:28:11,345 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-26 10:28:11,347 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-27 03:08:59,896 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-27 03:08:59,928 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-27 03:08:59,935 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-27 03:09:00,334 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-27 03:09:00,460 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-27 03:09:00,460 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-27 03:09:00,463 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-27 03:09:00,463 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-27 03:09:00,706 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-27 03:09:00,782 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-27 03:09:00,791 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-27 03:09:00,812 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-27 03:09:00,838 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-27 03:09:00,841 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-27 03:09:00,841 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-27 03:09:00,841 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-27 03:09:00,989 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-27 03:09:00,990 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-27 03:09:01,011 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-27 03:09:01,011 INFO org.mortbay.log: jetty-6.1.26
2020-08-27 03:09:01,379 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-27 03:09:01,415 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 03:09:01,415 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 03:09:01,464 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-27 03:09:01,465 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-27 03:09:01,467 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-27 03:09:01,506 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-27 03:09:01,506 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-27 03:09:01,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-27 03:09:01,508 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 27 03:09:01
2020-08-27 03:09:01,516 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-27 03:09:01,516 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 03:09:01,517 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-27 03:09:01,517 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-27 03:09:01,526 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-27 03:09:01,526 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-27 03:09:01,526 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-27 03:09:01,526 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-27 03:09:01,526 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-27 03:09:01,527 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-27 03:09:01,527 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-27 03:09:01,527 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-27 03:09:01,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-27 03:09:01,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-27 03:09:01,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-27 03:09:01,535 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-27 03:09:01,537 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-27 03:09:01,685 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-27 03:09:01,686 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 03:09:01,686 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-27 03:09:01,686 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-27 03:09:01,687 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-27 03:09:01,687 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-27 03:09:01,687 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-27 03:09:01,687 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-27 03:09:01,696 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-27 03:09:01,696 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 03:09:01,696 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-27 03:09:01,697 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-27 03:09:01,698 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-27 03:09:01,698 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-27 03:09:01,698 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-27 03:09:01,703 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-27 03:09:01,703 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-27 03:09:01,703 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-27 03:09:01,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-27 03:09:01,707 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-27 03:09:01,709 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-27 03:09:01,709 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 03:09:01,710 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-27 03:09:01,710 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-27 03:09:01,740 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 2071@producer.c.poetic-set-285601.internal
2020-08-27 03:09:01,822 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-27 03:09:02,004 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000004 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000004-0000000000000000011
2020-08-27 03:09:02,012 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000003, cpktTxId=0000000000000000003)
2020-08-27 03:09:02,077 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-08-27 03:09:02,107 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-27 03:09:02,107 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 3 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000003
2020-08-27 03:09:02,108 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5c153b9e expecting start txid #4
2020-08-27 03:09:02,108 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000004-0000000000000000011
2020-08-27 03:09:02,110 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000004-0000000000000000011' to transaction ID 4
2020-08-27 03:09:02,136 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000004-0000000000000000011 of size 1048576 edits # 8 loaded in 0 seconds
2020-08-27 03:09:02,136 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-27 03:09:02,140 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 4 milliseconds
name space=3
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-27 03:09:02,140 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2020-08-27 03:09:02,140 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2020-08-27 03:09:02,145 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000011 using no compression
2020-08-27 03:09:02,185 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000011 of size 474 bytes saved in 0 seconds.
2020-08-27 03:09:02,238 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 3
2020-08-27 03:09:02,238 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-08-27 03:09:02,249 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 12
2020-08-27 03:09:02,365 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-27 03:09:02,365 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 653 msecs
2020-08-27 03:09:02,534 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-27 03:09:02,540 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-27 03:09:02,551 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-27 03:09:02,682 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-27 03:09:02,693 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 03:09:02,694 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 03:09:02,694 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-27 03:09:02,694 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-08-27 03:09:02,694 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-27 03:09:02,694 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-27 03:09:02,700 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 03:09:02,710 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-27 03:09:02,710 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-27 03:09:02,710 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-27 03:09:02,710 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-27 03:09:02,710 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-27 03:09:02,710 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 16 msec
2020-08-27 03:09:02,729 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-27 03:09:02,731 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-27 03:09:02,735 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-27 03:09:02,736 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-27 03:09:02,741 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-27 03:09:09,809 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-27 03:09:09,809 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 03:09:09,810 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-27 03:09:09,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 03:09:09,922 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-27 03:09:09,965 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-27 03:09:09,965 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 03:09:09,965 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-27 03:09:10,015 INFO BlockStateChange: BLOCK* processReport 0x158e1890146: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 12 msecs
2020-08-27 03:09:10,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 03:09:10,033 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-27 03:09:10,100 INFO BlockStateChange: BLOCK* processReport 0x158b35b7a62: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 14 msecs
2020-08-27 03:11:47,604 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-27 03:11:47,607 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-27 04:23:02,927 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-27 04:23:02,940 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-27 04:23:02,950 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-27 04:23:03,325 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-27 04:23:03,453 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-27 04:23:03,454 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-27 04:23:03,456 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-27 04:23:03,457 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-27 04:23:03,734 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-27 04:23:03,791 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-27 04:23:03,801 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-27 04:23:03,820 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-27 04:23:03,827 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-27 04:23:03,832 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-27 04:23:03,832 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-27 04:23:03,832 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-27 04:23:03,984 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-27 04:23:03,985 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-27 04:23:04,001 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-27 04:23:04,001 INFO org.mortbay.log: jetty-6.1.26
2020-08-27 04:23:04,154 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-27 04:23:04,236 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 04:23:04,236 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 04:23:04,280 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-27 04:23:04,281 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-27 04:23:04,283 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-27 04:23:04,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-27 04:23:04,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-27 04:23:04,325 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-27 04:23:04,326 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 27 04:23:04
2020-08-27 04:23:04,332 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-27 04:23:04,332 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:23:04,333 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-27 04:23:04,333 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-27 04:23:04,341 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-27 04:23:04,349 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-27 04:23:04,349 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-27 04:23:04,349 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-27 04:23:04,349 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-27 04:23:04,350 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-27 04:23:04,428 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-27 04:23:04,428 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:23:04,428 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-27 04:23:04,428 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-27 04:23:04,429 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-27 04:23:04,429 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-27 04:23:04,429 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-27 04:23:04,429 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-27 04:23:04,438 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-27 04:23:04,438 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:23:04,439 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-27 04:23:04,439 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-27 04:23:04,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-27 04:23:04,444 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-27 04:23:04,445 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-27 04:23:04,448 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-27 04:23:04,448 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-27 04:23:04,448 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-27 04:23:04,452 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-27 04:23:04,453 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-27 04:23:04,455 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-27 04:23:04,455 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:23:04,455 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-27 04:23:04,455 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-27 04:23:04,504 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 3593@producer.c.poetic-set-285601.internal
2020-08-27 04:23:04,578 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-27 04:23:04,705 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000012 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000012-0000000000000000012
2020-08-27 04:23:04,712 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000011, cpktTxId=0000000000000000011)
2020-08-27 04:23:04,745 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2020-08-27 04:23:04,790 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-27 04:23:04,790 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 11 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000011
2020-08-27 04:23:04,790 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #12
2020-08-27 04:23:04,790 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000012-0000000000000000012
2020-08-27 04:23:04,792 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000012-0000000000000000012' to transaction ID 12
2020-08-27 04:23:04,794 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000012-0000000000000000012 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-27 04:23:04,794 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-27 04:23:04,802 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 8 milliseconds
name space=3
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-27 04:23:04,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2020-08-27 04:23:04,802 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2020-08-27 04:23:04,806 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000012 using no compression
2020-08-27 04:23:04,843 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000012 of size 474 bytes saved in 0 seconds.
2020-08-27 04:23:04,868 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 11
2020-08-27 04:23:04,868 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000003, cpktTxId=0000000000000000003)
2020-08-27 04:23:04,879 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 13
2020-08-27 04:23:04,995 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-27 04:23:04,995 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 536 msecs
2020-08-27 04:23:05,182 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-27 04:23:05,194 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-27 04:23:05,211 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-27 04:23:05,274 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-27 04:23:05,286 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 04:23:05,286 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 04:23:05,286 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-27 04:23:05,287 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-08-27 04:23:05,287 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-27 04:23:05,287 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-27 04:23:05,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:23:05,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-27 04:23:05,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-27 04:23:05,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-27 04:23:05,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-27 04:23:05,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-27 04:23:05,306 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 19 msec
2020-08-27 04:23:05,339 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-27 04:23:05,340 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-27 04:23:05,343 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-27 04:23:05,343 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-27 04:23:05,349 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-27 04:23:09,754 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-27 04:23:09,754 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:23:09,755 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-27 04:23:09,771 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-27 04:23:09,771 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:23:09,771 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-27 04:23:09,838 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:23:09,838 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-27 04:23:09,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:23:09,845 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-27 04:23:09,882 INFO BlockStateChange: BLOCK* processReport 0x5629fa63220: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 2 msecs
2020-08-27 04:23:09,894 INFO BlockStateChange: BLOCK* processReport 0x5626cc331bb: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 0 msecs
2020-08-27 04:23:10,113 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-27 04:23:10,114 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:23:10,114 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-27 04:23:10,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:23:10,165 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-27 04:23:10,185 INFO BlockStateChange: BLOCK* processReport 0x56277f958ea: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-27 04:24:14,638 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-27 04:24:14,638 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-27 04:24:14,638 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 13
2020-08-27 04:24:14,639 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2020-08-27 04:24:14,640 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 11 
2020-08-27 04:24:14,641 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000013 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000013-0000000000000000014
2020-08-27 04:24:14,641 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 15
2020-08-27 04:24:15,572 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-27 04:24:15,572 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000014 size 474 bytes.
2020-08-27 04:24:15,575 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 12
2020-08-27 04:24:15,575 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000011, cpktTxId=0000000000000000011)
2020-08-27 04:53:59,056 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-27 04:53:59,058 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-27 04:55:02,372 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-27 04:55:02,383 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-27 04:55:02,389 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-27 04:55:02,799 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-27 04:55:02,943 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-27 04:55:02,943 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-27 04:55:02,946 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-27 04:55:02,947 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-27 04:55:03,191 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-27 04:55:03,250 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-27 04:55:03,259 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-27 04:55:03,273 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-27 04:55:03,278 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-27 04:55:03,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-27 04:55:03,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-27 04:55:03,281 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-27 04:55:03,426 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-27 04:55:03,427 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-27 04:55:03,443 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-27 04:55:03,443 INFO org.mortbay.log: jetty-6.1.26
2020-08-27 04:55:03,588 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-27 04:55:03,624 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 04:55:03,624 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 04:55:03,672 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-27 04:55:03,673 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-27 04:55:03,674 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-27 04:55:03,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-27 04:55:03,714 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-27 04:55:03,716 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-27 04:55:03,716 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 27 04:55:03
2020-08-27 04:55:03,722 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-27 04:55:03,722 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:55:03,724 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-27 04:55:03,724 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-27 04:55:03,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-27 04:55:03,741 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-27 04:55:03,741 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-27 04:55:03,741 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-27 04:55:03,741 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-27 04:55:03,743 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-27 04:55:03,819 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-27 04:55:03,819 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:55:03,819 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-27 04:55:03,819 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-27 04:55:03,820 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-27 04:55:03,820 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-27 04:55:03,820 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-27 04:55:03,820 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-27 04:55:03,830 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-27 04:55:03,830 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:55:03,831 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-27 04:55:03,831 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-27 04:55:03,832 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-27 04:55:03,832 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-27 04:55:03,832 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-27 04:55:03,836 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-27 04:55:03,837 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-27 04:55:03,837 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-27 04:55:03,843 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-27 04:55:03,844 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-27 04:55:03,846 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-27 04:55:03,846 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 04:55:03,846 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-27 04:55:03,846 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-27 04:55:03,862 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 4883@producer.c.poetic-set-285601.internal
2020-08-27 04:55:03,930 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-27 04:55:04,065 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000015 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000015-0000000000000000015
2020-08-27 04:55:04,072 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000014, cpktTxId=0000000000000000014)
2020-08-27 04:55:04,106 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2020-08-27 04:55:04,154 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-27 04:55:04,154 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 14 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000014
2020-08-27 04:55:04,154 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #15
2020-08-27 04:55:04,155 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000015-0000000000000000015
2020-08-27 04:55:04,156 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000015-0000000000000000015' to transaction ID 15
2020-08-27 04:55:04,158 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000015-0000000000000000015 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-27 04:55:04,159 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-27 04:55:04,166 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 7 milliseconds
name space=3
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-27 04:55:04,166 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-27 04:55:04,168 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 16
2020-08-27 04:55:04,305 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-27 04:55:04,305 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 456 msecs
2020-08-27 04:55:04,482 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-27 04:55:04,489 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-27 04:55:04,506 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-27 04:55:04,577 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-27 04:55:04,586 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 04:55:04,587 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 04:55:04,587 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-27 04:55:04,587 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-08-27 04:55:04,587 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-27 04:55:04,587 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-27 04:55:04,595 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:55:04,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-27 04:55:04,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-27 04:55:04,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-27 04:55:04,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-27 04:55:04,624 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-27 04:55:04,624 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 35 msec
2020-08-27 04:55:04,633 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-27 04:55:04,635 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-27 04:55:04,637 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-27 04:55:04,637 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-27 04:55:04,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-27 04:55:09,243 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-27 04:55:09,243 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:55:09,244 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-27 04:55:09,307 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-27 04:55:09,308 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:55:09,308 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-27 04:55:09,316 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-27 04:55:09,316 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:55:09,316 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-27 04:55:09,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:55:09,321 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-27 04:55:09,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:55:09,350 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-27 04:55:09,371 INFO BlockStateChange: BLOCK* processReport 0x72189db95a2: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 4 msecs
2020-08-27 04:55:09,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 04:55:09,371 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-27 04:55:09,391 INFO BlockStateChange: BLOCK* processReport 0x72150506992: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-27 04:55:09,397 INFO BlockStateChange: BLOCK* processReport 0x721587928b1: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 4 msecs
2020-08-27 04:56:14,153 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-27 04:56:14,153 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-27 04:56:14,153 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 16
2020-08-27 04:56:14,153 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 27 
2020-08-27 04:56:14,155 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 28 
2020-08-27 04:56:14,156 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000016 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000016-0000000000000000017
2020-08-27 04:56:14,156 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 18
2020-08-27 04:56:15,134 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-27 04:56:15,135 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000017 size 474 bytes.
2020-08-27 04:56:15,140 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 14
2020-08-27 04:56:15,140 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000012, cpktTxId=0000000000000000012)
2020-08-27 05:56:15,611 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-27 05:56:15,611 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-27 05:56:15,611 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 18
2020-08-27 05:56:15,612 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 8 
2020-08-27 05:56:15,632 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 27 
2020-08-27 05:56:15,633 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000018 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000018-0000000000000000019
2020-08-27 05:56:15,633 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 20
2020-08-27 05:56:15,699 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-27 05:56:15,699 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000019 size 474 bytes.
2020-08-27 05:56:15,704 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 17
2020-08-27 05:56:15,704 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000014, cpktTxId=0000000000000000014)
2020-08-27 05:58:20,232 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-27 05:58:20,235 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-27 05:59:11,330 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-27 05:59:11,344 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-27 05:59:11,353 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-27 05:59:11,759 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-27 05:59:11,885 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-27 05:59:11,885 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-27 05:59:11,888 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-27 05:59:11,888 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-27 05:59:12,108 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-27 05:59:12,167 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-27 05:59:12,176 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-27 05:59:12,192 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-27 05:59:12,197 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-27 05:59:12,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-27 05:59:12,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-27 05:59:12,200 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-27 05:59:12,348 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-27 05:59:12,350 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-27 05:59:12,366 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-27 05:59:12,366 INFO org.mortbay.log: jetty-6.1.26
2020-08-27 05:59:12,521 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-27 05:59:12,553 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 05:59:12,553 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 05:59:12,598 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-27 05:59:12,599 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-27 05:59:12,601 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-27 05:59:12,641 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-27 05:59:12,641 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-27 05:59:12,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-27 05:59:12,643 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 27 05:59:12
2020-08-27 05:59:12,652 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-27 05:59:12,653 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 05:59:12,654 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-27 05:59:12,654 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-27 05:59:12,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-27 05:59:12,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-27 05:59:12,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-27 05:59:12,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-27 05:59:12,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-27 05:59:12,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-27 05:59:12,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-27 05:59:12,663 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-27 05:59:12,671 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-27 05:59:12,671 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-27 05:59:12,671 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-27 05:59:12,671 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-27 05:59:12,673 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-27 05:59:12,749 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-27 05:59:12,749 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 05:59:12,749 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-27 05:59:12,749 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-27 05:59:12,750 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-27 05:59:12,750 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-27 05:59:12,750 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-27 05:59:12,750 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-27 05:59:12,759 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-27 05:59:12,759 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 05:59:12,759 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-27 05:59:12,759 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-27 05:59:12,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-27 05:59:12,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-27 05:59:12,761 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-27 05:59:12,764 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-27 05:59:12,764 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-27 05:59:12,764 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-27 05:59:12,768 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-27 05:59:12,768 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-27 05:59:12,771 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-27 05:59:12,771 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 05:59:12,771 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-27 05:59:12,771 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-27 05:59:12,785 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 6914@producer.c.poetic-set-285601.internal
2020-08-27 05:59:12,844 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-27 05:59:13,003 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000020 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000020-0000000000000000020
2020-08-27 05:59:13,013 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000019, cpktTxId=0000000000000000019)
2020-08-27 05:59:13,058 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2020-08-27 05:59:13,103 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-27 05:59:13,104 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 19 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000019
2020-08-27 05:59:13,104 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #20
2020-08-27 05:59:13,104 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000020-0000000000000000020
2020-08-27 05:59:13,106 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000020-0000000000000000020' to transaction ID 20
2020-08-27 05:59:13,108 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000020-0000000000000000020 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-27 05:59:13,108 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-27 05:59:13,116 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 8 milliseconds
name space=3
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-27 05:59:13,116 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-27 05:59:13,118 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 21
2020-08-27 05:59:13,235 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-27 05:59:13,235 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 461 msecs
2020-08-27 05:59:13,421 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-27 05:59:13,429 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-27 05:59:13,446 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-27 05:59:13,532 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-27 05:59:13,547 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 05:59:13,547 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 05:59:13,548 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-27 05:59:13,548 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-08-27 05:59:13,548 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-27 05:59:13,548 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-27 05:59:13,557 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 05:59:13,602 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-27 05:59:13,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-27 05:59:13,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-27 05:59:13,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-27 05:59:13,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-27 05:59:13,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-27 05:59:13,602 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 53 msec
2020-08-27 05:59:13,604 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-27 05:59:13,623 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-27 05:59:13,623 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-27 05:59:13,628 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-27 05:59:18,118 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-27 05:59:18,118 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 05:59:18,119 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-27 05:59:18,183 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-27 05:59:18,183 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 05:59:18,183 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-27 05:59:18,200 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-27 05:59:18,200 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 05:59:18,200 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-27 05:59:18,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 05:59:18,208 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-27 05:59:18,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 05:59:18,229 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-27 05:59:18,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 05:59:18,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-27 05:59:18,272 INFO BlockStateChange: BLOCK* processReport 0xaa1ae521d68: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 3 msecs
2020-08-27 05:59:18,273 INFO BlockStateChange: BLOCK* processReport 0xaa1746e725b: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-27 05:59:18,290 INFO BlockStateChange: BLOCK* processReport 0xaa17be2c918: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 7 msecs
2020-08-27 06:00:22,979 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-27 06:00:22,979 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-27 06:00:22,979 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 21
2020-08-27 06:00:22,979 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2020-08-27 06:00:22,981 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 11 
2020-08-27 06:00:22,983 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000021 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000021-0000000000000000022
2020-08-27 06:00:22,983 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 23
2020-08-27 06:00:23,973 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-27 06:00:23,973 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000022 size 474 bytes.
2020-08-27 06:00:23,982 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 19
2020-08-27 06:00:23,983 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000017, cpktTxId=0000000000000000017)
2020-08-27 06:04:11,657 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-27 06:04:11,659 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-27 06:05:11,528 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-27 06:05:11,539 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-27 06:05:11,545 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-27 06:05:11,927 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-27 06:05:12,052 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-27 06:05:12,052 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-27 06:05:12,055 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-27 06:05:12,055 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-27 06:05:12,287 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-27 06:05:12,347 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-27 06:05:12,357 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-27 06:05:12,379 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-27 06:05:12,386 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-27 06:05:12,392 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-27 06:05:12,392 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-27 06:05:12,392 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-27 06:05:12,540 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-27 06:05:12,541 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-27 06:05:12,557 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-27 06:05:12,557 INFO org.mortbay.log: jetty-6.1.26
2020-08-27 06:05:12,704 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-27 06:05:12,736 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 06:05:12,736 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 06:05:12,781 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-27 06:05:12,783 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-27 06:05:12,784 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-27 06:05:12,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-27 06:05:12,823 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-27 06:05:12,824 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-27 06:05:12,825 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 27 06:05:12
2020-08-27 06:05:12,831 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-27 06:05:12,831 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:05:12,833 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-27 06:05:12,833 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-27 06:05:12,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-27 06:05:12,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-27 06:05:12,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-27 06:05:12,840 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-27 06:05:12,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-27 06:05:12,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-27 06:05:12,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-27 06:05:12,841 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-27 06:05:12,849 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-27 06:05:12,849 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-27 06:05:12,849 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-27 06:05:12,849 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-27 06:05:12,851 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-27 06:05:12,925 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-27 06:05:12,925 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:05:12,926 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-27 06:05:12,926 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-27 06:05:12,926 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-27 06:05:12,927 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-27 06:05:12,927 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-27 06:05:12,927 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-27 06:05:12,936 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-27 06:05:12,936 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:05:12,936 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-27 06:05:12,936 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-27 06:05:12,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-27 06:05:12,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-27 06:05:12,941 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-27 06:05:12,945 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-27 06:05:12,945 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-27 06:05:12,945 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-27 06:05:12,950 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-27 06:05:12,950 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-27 06:05:12,952 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-27 06:05:12,952 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:05:12,953 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-27 06:05:12,953 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-27 06:05:12,966 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 8125@producer.c.poetic-set-285601.internal
2020-08-27 06:05:13,024 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-27 06:05:13,163 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000023 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000023-0000000000000000023
2020-08-27 06:05:13,173 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000022, cpktTxId=0000000000000000022)
2020-08-27 06:05:13,217 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2020-08-27 06:05:13,267 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-27 06:05:13,267 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 22 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000022
2020-08-27 06:05:13,267 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3401a114 expecting start txid #23
2020-08-27 06:05:13,268 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000023-0000000000000000023
2020-08-27 06:05:13,269 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000023-0000000000000000023' to transaction ID 23
2020-08-27 06:05:13,272 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000023-0000000000000000023 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-27 06:05:13,272 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-27 06:05:13,279 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 7 milliseconds
name space=3
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-27 06:05:13,279 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-27 06:05:13,282 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 24
2020-08-27 06:05:13,409 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-27 06:05:13,409 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 454 msecs
2020-08-27 06:05:13,577 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-27 06:05:13,585 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-27 06:05:13,600 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-27 06:05:13,674 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-27 06:05:13,687 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 06:05:13,687 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 06:05:13,687 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-27 06:05:13,688 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-08-27 06:05:13,688 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-27 06:05:13,689 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-27 06:05:13,698 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:05:13,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-27 06:05:13,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-27 06:05:13,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-27 06:05:13,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-27 06:05:13,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-27 06:05:13,745 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 55 msec
2020-08-27 06:05:13,745 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-27 06:05:13,747 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-27 06:05:13,751 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-27 06:05:13,751 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-27 06:05:13,756 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-27 06:05:18,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-27 06:05:18,199 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:05:18,200 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-27 06:05:18,249 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-27 06:05:18,250 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:05:18,250 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-27 06:05:18,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:05:18,283 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-27 06:05:18,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:05:18,294 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-27 06:05:18,332 INFO BlockStateChange: BLOCK* processReport 0xaf583925621: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 2 msecs
2020-08-27 06:05:18,332 INFO BlockStateChange: BLOCK* processReport 0xaf54925e9e5: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 0 msecs
2020-08-27 06:05:18,362 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-27 06:05:18,362 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:05:18,362 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-27 06:05:18,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:05:18,408 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-27 06:05:18,440 INFO BlockStateChange: BLOCK* processReport 0xaf557281eb9: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-27 06:06:23,147 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-27 06:06:23,147 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-27 06:06:23,147 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 24
2020-08-27 06:06:23,148 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 17 
2020-08-27 06:06:23,149 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 19 
2020-08-27 06:06:23,150 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000024 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000024-0000000000000000025
2020-08-27 06:06:23,150 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 26
2020-08-27 06:06:24,082 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2020-08-27 06:06:24,082 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000025 size 474 bytes.
2020-08-27 06:06:24,086 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 22
2020-08-27 06:06:24,086 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000019, cpktTxId=0000000000000000019)
2020-08-27 06:15:50,155 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-27 06:15:50,157 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-27 06:47:08,061 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/spark-2.3.1-yarn-shuffle.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-27 06:47:08,075 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-27 06:47:08,083 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-27 06:47:08,450 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-27 06:47:08,568 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-27 06:47:08,568 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-27 06:47:08,571 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-27 06:47:08,571 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-27 06:47:08,860 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-27 06:47:08,916 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-27 06:47:08,925 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-27 06:47:08,945 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-27 06:47:08,953 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-27 06:47:08,956 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-27 06:47:08,957 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-27 06:47:08,957 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-27 06:47:09,112 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-27 06:47:09,114 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-27 06:47:09,130 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-27 06:47:09,130 INFO org.mortbay.log: jetty-6.1.26
2020-08-27 06:47:09,320 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-27 06:47:09,375 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 06:47:09,375 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-27 06:47:09,408 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-27 06:47:09,409 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-27 06:47:09,411 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-27 06:47:09,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-27 06:47:09,447 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-27 06:47:09,448 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-27 06:47:09,449 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 27 06:47:09
2020-08-27 06:47:09,451 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-27 06:47:09,451 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:47:09,462 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-27 06:47:09,462 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-27 06:47:09,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-27 06:47:09,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-27 06:47:09,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-27 06:47:09,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-27 06:47:09,469 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-27 06:47:09,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-27 06:47:09,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-27 06:47:09,470 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-27 06:47:09,479 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-27 06:47:09,479 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-27 06:47:09,479 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-27 06:47:09,479 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-27 06:47:09,481 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-27 06:47:09,552 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-27 06:47:09,552 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:47:09,553 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-27 06:47:09,553 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-27 06:47:09,554 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-27 06:47:09,554 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-27 06:47:09,554 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-27 06:47:09,554 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-27 06:47:09,563 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-27 06:47:09,563 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:47:09,563 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-27 06:47:09,563 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-27 06:47:09,565 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-27 06:47:09,565 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-27 06:47:09,565 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-27 06:47:09,569 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-27 06:47:09,569 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-27 06:47:09,569 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-27 06:47:09,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-27 06:47:09,573 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-27 06:47:09,575 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-27 06:47:09,575 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-27 06:47:09,575 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-27 06:47:09,575 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-27 06:47:09,604 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 10052@producer.c.poetic-set-285601.internal
2020-08-27 06:47:09,674 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-27 06:47:09,856 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000026 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000026-0000000000000000026
2020-08-27 06:47:09,867 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000025, cpktTxId=0000000000000000025)
2020-08-27 06:47:09,912 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2020-08-27 06:47:09,960 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-27 06:47:09,960 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 25 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000025
2020-08-27 06:47:09,960 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4233e892 expecting start txid #26
2020-08-27 06:47:09,960 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000026-0000000000000000026
2020-08-27 06:47:09,962 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000026-0000000000000000026' to transaction ID 26
2020-08-27 06:47:09,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000026-0000000000000000026 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-27 06:47:09,964 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-27 06:47:09,972 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 8 milliseconds
name space=3
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-27 06:47:09,973 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-27 06:47:09,976 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 27
2020-08-27 06:47:10,133 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-27 06:47:10,134 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 555 msecs
2020-08-27 06:47:10,316 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-27 06:47:10,328 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-27 06:47:10,343 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-27 06:47:10,415 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-27 06:47:10,424 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 06:47:10,425 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-27 06:47:10,425 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-27 06:47:10,425 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-08-27 06:47:10,425 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-27 06:47:10,425 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-27 06:47:10,436 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:47:10,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-27 06:47:10,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-27 06:47:10,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-27 06:47:10,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-27 06:47:10,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-27 06:47:10,472 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 40 msec
2020-08-27 06:47:10,480 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-27 06:47:10,482 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-27 06:47:10,486 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-27 06:47:10,486 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-27 06:47:10,494 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-27 06:47:14,981 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-27 06:47:14,981 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:47:14,982 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-27 06:47:15,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:47:15,071 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-27 06:47:15,090 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-27 06:47:15,090 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:47:15,090 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-27 06:47:15,118 INFO BlockStateChange: BLOCK* processReport 0xd3f7fd50029: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-27 06:47:15,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:47:15,134 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-27 06:47:15,153 INFO BlockStateChange: BLOCK* processReport 0xd3f484bc36e: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 0 msecs
2020-08-27 06:47:15,450 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-27 06:47:15,450 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:47:15,450 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-27 06:47:15,501 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-27 06:47:15,501 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-27 06:47:15,536 INFO BlockStateChange: BLOCK* processReport 0xd3f65c9de82: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-27 06:47:53,276 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-27 06:47:53,279 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-31 05:21:26,428 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/spark-2.3.1-yarn-shuffle.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-31 05:21:26,466 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-31 05:21:26,476 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-31 05:21:26,922 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-31 05:21:27,209 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-31 05:21:27,209 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-31 05:21:27,212 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-31 05:21:27,213 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-31 05:21:27,499 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-31 05:21:27,575 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-31 05:21:27,584 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-31 05:21:27,644 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-31 05:21:27,661 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-31 05:21:27,664 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-31 05:21:27,664 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-31 05:21:27,664 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-31 05:21:27,814 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-31 05:21:27,816 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-31 05:21:27,838 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-31 05:21:27,838 INFO org.mortbay.log: jetty-6.1.26
2020-08-31 05:21:28,204 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-31 05:21:28,300 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 05:21:28,300 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 05:21:28,352 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-31 05:21:28,353 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-31 05:21:28,356 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-31 05:21:28,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-31 05:21:28,412 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-31 05:21:28,413 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-31 05:21:28,414 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 31 05:21:28
2020-08-31 05:21:28,416 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-31 05:21:28,416 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 05:21:28,418 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-31 05:21:28,418 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-31 05:21:28,425 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-31 05:21:28,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-31 05:21:28,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-31 05:21:28,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-31 05:21:28,431 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-31 05:21:28,432 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-31 05:21:28,574 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-31 05:21:28,574 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 05:21:28,574 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-31 05:21:28,574 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-31 05:21:28,575 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-31 05:21:28,575 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-31 05:21:28,575 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-31 05:21:28,575 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-31 05:21:28,583 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-31 05:21:28,583 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 05:21:28,584 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-31 05:21:28,584 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-31 05:21:28,585 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-31 05:21:28,586 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-31 05:21:28,586 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-31 05:21:28,592 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-31 05:21:28,592 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-31 05:21:28,593 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-31 05:21:28,598 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-31 05:21:28,598 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-31 05:21:28,600 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-31 05:21:28,601 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 05:21:28,601 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-31 05:21:28,601 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-31 05:21:28,640 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 1376@producer.c.poetic-set-285601.internal
2020-08-31 05:21:28,728 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-31 05:21:28,862 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000027 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000027-0000000000000000027
2020-08-31 05:21:28,872 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000025, cpktTxId=0000000000000000025)
2020-08-31 05:21:28,908 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 3 INodes.
2020-08-31 05:21:28,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-31 05:21:28,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 25 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000025
2020-08-31 05:21:28,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@319bc845 expecting start txid #26
2020-08-31 05:21:28,955 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000026-0000000000000000026
2020-08-31 05:21:28,957 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000026-0000000000000000026' to transaction ID 26
2020-08-31 05:21:28,998 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000026-0000000000000000026 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-31 05:21:28,998 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4c5474f5 expecting start txid #27
2020-08-31 05:21:28,998 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000027-0000000000000000027
2020-08-31 05:21:28,998 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000027-0000000000000000027' to transaction ID 26
2020-08-31 05:21:29,000 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000027-0000000000000000027 of size 1048576 edits # 1 loaded in 0 seconds
2020-08-31 05:21:29,000 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-31 05:21:29,008 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 8 milliseconds
name space=3
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-31 05:21:29,008 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2020-08-31 05:21:29,008 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2020-08-31 05:21:29,035 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000027 using no compression
2020-08-31 05:21:29,073 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000027 of size 474 bytes saved in 0 seconds.
2020-08-31 05:21:29,103 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 25
2020-08-31 05:21:29,103 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000022, cpktTxId=0000000000000000022)
2020-08-31 05:21:29,116 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 28
2020-08-31 05:21:29,239 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-31 05:21:29,240 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 636 msecs
2020-08-31 05:21:29,431 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-31 05:21:29,443 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-31 05:21:29,462 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-31 05:21:29,578 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-31 05:21:29,591 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 05:21:29,591 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 05:21:29,591 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-31 05:21:29,592 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-08-31 05:21:29,592 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-31 05:21:29,592 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-31 05:21:29,600 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 05:21:29,625 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-31 05:21:29,625 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-31 05:21:29,625 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-31 05:21:29,625 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-31 05:21:29,625 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-31 05:21:29,625 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 32 msec
2020-08-31 05:21:29,644 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-31 05:21:29,645 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-31 05:21:29,648 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-31 05:21:29,648 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-31 05:21:29,653 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-31 05:21:37,518 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-31 05:21:37,518 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 05:21:37,519 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-31 05:21:37,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 05:21:37,652 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-31 05:21:37,710 INFO BlockStateChange: BLOCK* processReport 0xb047bbfaed: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 2 msecs
2020-08-31 05:21:38,471 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-31 05:21:38,472 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 05:21:38,472 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-31 05:21:38,514 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 05:21:38,515 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-31 05:21:38,536 INFO BlockStateChange: BLOCK* processReport 0xb0ac37555b: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 0 msecs
2020-08-31 05:21:38,592 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-31 05:21:38,592 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 05:21:38,592 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-31 05:21:38,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 05:21:38,642 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-31 05:21:38,643 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2020-08-31 05:21:38,644 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) For more information, please enable DEBUG log level on org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy
2020-08-31 05:21:38,644 WARN org.apache.hadoop.hdfs.protocol.BlockStoragePolicy: Failed to place enough replicas: expected size is 1 but only 0 storage types can be selected (replication=3, selected=[], unavailable=[DISK, ARCHIVE], removed=[DISK], policy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]})
2020-08-31 05:21:38,644 WARN org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicy: Failed to place enough replicas, still in need of 1 to reach 3 (unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}, newBlock=false) All required storage types are unavailable:  unavailableStorages=[DISK, ARCHIVE], storagePolicy=BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}
2020-08-31 05:21:38,671 INFO BlockStateChange: BLOCK* processReport 0xb07701460e: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 0 msecs
2020-08-31 05:33:47,513 INFO BlockStateChange: BLOCK* processReport 0x15a6717f84f: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-31 06:09:39,118 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-31 06:09:39,119 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-31 06:09:39,119 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 28
2020-08-31 06:09:39,119 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 12 
2020-08-31 06:09:39,133 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 26 
2020-08-31 06:09:39,135 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000028 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000028-0000000000000000029
2020-08-31 06:09:39,135 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 30
2020-08-31 06:09:40,295 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.01s at 0.00 KB/s
2020-08-31 06:09:40,295 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000029 size 474 bytes.
2020-08-31 06:09:40,298 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 27
2020-08-31 06:09:40,298 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000025, cpktTxId=0000000000000000025)
2020-08-31 06:10:24,997 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0001/__spark_libs__1403158082489372304.zip
2020-08-31 06:10:26,624 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:10:26,647 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0001/__spark_libs__1403158082489372304.zip
2020-08-31 06:10:26,650 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741826_1002 size 67108864
2020-08-31 06:10:26,650 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741826_1002 size 67108864
2020-08-31 06:10:28,041 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:28,041 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:28,063 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0001/__spark_libs__1403158082489372304.zip
2020-08-31 06:10:28,064 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741827_1003 size 67108864
2020-08-31 06:10:29,191 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:10:29,193 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:10:29,196 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:10:29,199 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0001/__spark_libs__1403158082489372304.zip
2020-08-31 06:10:29,742 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:29,745 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:29,769 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:29,779 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0001/__spark_libs__1403158082489372304.zip is closed by DFSClient_NONMAPREDUCE_-435123652_1
2020-08-31 06:10:30,535 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0001/__spark_conf__.zip
2020-08-31 06:10:30,592 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:30,621 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:30,622 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:10:30,625 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-435123652_1
2020-08-31 06:10:48,866 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 30 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 17 SyncTimes(ms): 101 
2020-08-31 06:10:48,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:10:48,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741826_1002 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:10:48,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741827_1003 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 06:10:48,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741828_1004 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:10:48,868 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741829_1005 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:10:50,965 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741826_1002, blk_1073741827_1003, blk_1073741828_1004, blk_1073741829_1005, blk_1073741830_1006]
2020-08-31 06:10:53,965 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741826_1002, blk_1073741827_1003, blk_1073741828_1004, blk_1073741829_1005, blk_1073741830_1006]
2020-08-31 06:10:56,966 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741826_1002, blk_1073741827_1003, blk_1073741828_1004, blk_1073741829_1005, blk_1073741830_1006]
2020-08-31 06:20:16,349 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 31 Total time for transactions(ms): 25 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 103 
2020-08-31 06:20:20,607 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0002/__spark_libs__7394770587211166824.zip
2020-08-31 06:20:22,137 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:22,137 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:22,145 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0002/__spark_libs__7394770587211166824.zip
2020-08-31 06:20:22,148 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741831_1007 size 67108864
2020-08-31 06:20:22,979 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:20:22,979 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:20:22,982 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:20:22,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0002/__spark_libs__7394770587211166824.zip
2020-08-31 06:20:23,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:23,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:23,538 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:23,548 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0002/__spark_libs__7394770587211166824.zip
2020-08-31 06:20:23,880 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:23,880 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:23,880 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:20:23,896 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0002/__spark_libs__7394770587211166824.zip is closed by DFSClient_NONMAPREDUCE_-1271358196_1
2020-08-31 06:20:24,634 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0002/__spark_conf__.zip
2020-08-31 06:20:24,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:20:24,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:20:24,722 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:20:24,738 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0002/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1271358196_1
2020-08-31 06:20:44,657 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741835_1011 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:20:44,657 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741831_1007 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:20:44,657 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741832_1008 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:20:44,657 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741833_1009 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:20:44,657 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741834_1010 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:20:45,052 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009, blk_1073741834_1010, blk_1073741835_1011]
2020-08-31 06:20:48,053 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009, blk_1073741834_1010, blk_1073741835_1011]
2020-08-31 06:20:51,053 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741831_1007, blk_1073741832_1008, blk_1073741833_1009, blk_1073741834_1010, blk_1073741835_1011]
2020-08-31 06:24:47,788 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 57 Total time for transactions(ms): 27 Number of transactions batched in Syncs: 0 Number of syncs: 34 SyncTimes(ms): 283 
2020-08-31 06:24:52,124 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0003/__spark_libs__195714997551041323.zip
2020-08-31 06:24:53,483 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0003/__spark_libs__195714997551041323.zip
2020-08-31 06:24:53,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741836_1012{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 67108864
2020-08-31 06:24:53,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741836_1012 size 67108864
2020-08-31 06:24:53,485 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741836_1012 size 67108864
2020-08-31 06:24:54,034 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:24:54,035 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:24:54,035 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:24:54,051 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0003/__spark_libs__195714997551041323.zip
2020-08-31 06:24:54,691 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:24:54,691 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:24:54,692 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:24:54,694 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0003/__spark_libs__195714997551041323.zip
2020-08-31 06:24:55,276 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:24:55,277 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:24:55,277 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:24:55,305 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0003/__spark_libs__195714997551041323.zip is closed by DFSClient_NONMAPREDUCE_-362845575_1
2020-08-31 06:24:55,900 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0003/__spark_conf__.zip
2020-08-31 06:24:55,950 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:24:55,954 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:24:55,955 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:24:55,968 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0003/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-362845575_1
2020-08-31 06:25:16,298 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741840_1016 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 06:25:16,298 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741836_1012 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:25:16,298 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741837_1013 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:25:16,298 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741838_1014 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:25:16,299 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741839_1015 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:25:18,124 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741840_1016, blk_1073741836_1012, blk_1073741837_1013, blk_1073741838_1014, blk_1073741839_1015]
2020-08-31 06:25:21,124 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741840_1016, blk_1073741836_1012, blk_1073741837_1013, blk_1073741838_1014, blk_1073741839_1015]
2020-08-31 06:25:24,125 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741840_1016, blk_1073741836_1012, blk_1073741837_1013, blk_1073741838_1014, blk_1073741839_1015]
2020-08-31 06:31:26,361 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 83 Total time for transactions(ms): 30 Number of transactions batched in Syncs: 0 Number of syncs: 50 SyncTimes(ms): 440 
2020-08-31 06:31:30,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0004/__spark_libs__6564989680654816169.zip
2020-08-31 06:31:32,119 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:32,119 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:32,119 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:32,151 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0004/__spark_libs__6564989680654816169.zip
2020-08-31 06:31:32,698 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:31:32,699 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:31:32,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:31:32,719 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0004/__spark_libs__6564989680654816169.zip
2020-08-31 06:31:33,540 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:33,541 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:33,543 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0004/__spark_libs__6564989680654816169.zip
2020-08-31 06:31:33,544 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741843_1019 size 67108864
2020-08-31 06:31:33,862 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:33,862 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:33,862 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:31:33,887 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0004/__spark_libs__6564989680654816169.zip is closed by DFSClient_NONMAPREDUCE_-1558679972_1
2020-08-31 06:31:34,567 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0004/__spark_conf__.zip
2020-08-31 06:31:34,670 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:31:34,672 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:31:34,674 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:31:34,688 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0004/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1558679972_1
2020-08-31 06:31:56,837 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741845_1021 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:31:56,837 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741841_1017 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:31:56,837 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741842_1018 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:31:56,837 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741843_1019 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:31:56,838 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741844_1020 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:31:57,177 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741841_1017, blk_1073741842_1018, blk_1073741843_1019, blk_1073741844_1020, blk_1073741845_1021]
2020-08-31 06:32:00,177 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741841_1017, blk_1073741842_1018, blk_1073741843_1019, blk_1073741844_1020, blk_1073741845_1021]
2020-08-31 06:32:03,177 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741841_1017, blk_1073741842_1018, blk_1073741843_1019, blk_1073741844_1020, blk_1073741845_1021]
2020-08-31 06:35:56,595 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 109 Total time for transactions(ms): 32 Number of transactions batched in Syncs: 0 Number of syncs: 66 SyncTimes(ms): 755 
2020-08-31 06:36:01,179 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0005/__spark_libs__4109017294097748316.zip
2020-08-31 06:36:02,385 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:02,385 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:02,387 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:02,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0005/__spark_libs__4109017294097748316.zip
2020-08-31 06:36:03,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:03,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:03,275 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0005/__spark_libs__4109017294097748316.zip
2020-08-31 06:36:03,276 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741847_1023 size 67108864
2020-08-31 06:36:04,138 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:04,139 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:04,151 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:04,153 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0005/__spark_libs__4109017294097748316.zip
2020-08-31 06:36:04,512 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:36:04,513 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:36:04,513 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:36:04,536 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0005/__spark_libs__4109017294097748316.zip is closed by DFSClient_NONMAPREDUCE_-1098200262_1
2020-08-31 06:36:05,183 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0005/__spark_conf__.zip
2020-08-31 06:36:05,206 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741850_1026{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:36:05,211 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741850_1026 size 183859
2020-08-31 06:36:05,219 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741850_1026 size 183859
2020-08-31 06:36:05,231 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0005/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1098200262_1
2020-08-31 06:36:27,308 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741850_1026 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:36:27,308 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741846_1022 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:36:27,308 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741847_1023 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:36:27,308 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741848_1024 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 06:36:27,308 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741849_1025 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:36:30,235 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741846_1022, blk_1073741847_1023, blk_1073741848_1024, blk_1073741849_1025, blk_1073741850_1026]
2020-08-31 06:36:33,235 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741846_1022, blk_1073741847_1023, blk_1073741848_1024, blk_1073741849_1025, blk_1073741850_1026]
2020-08-31 06:36:36,236 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741846_1022, blk_1073741847_1023, blk_1073741848_1024, blk_1073741849_1025, blk_1073741850_1026]
2020-08-31 06:45:14,110 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 135 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 0 Number of syncs: 82 SyncTimes(ms): 892 
2020-08-31 06:45:16,939 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0006/__spark_libs__4874354695191714472.zip
2020-08-31 06:45:17,564 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:17,565 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:17,567 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741851_1027{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:17,573 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0006/__spark_libs__4874354695191714472.zip
2020-08-31 06:45:17,857 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:45:17,857 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:45:17,858 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741852_1028{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:45:17,860 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0006/__spark_libs__4874354695191714472.zip
2020-08-31 06:45:18,160 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:45:18,164 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:45:18,164 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741853_1029{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:45:18,169 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0006/__spark_libs__4874354695191714472.zip
2020-08-31 06:45:18,349 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:18,350 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:18,352 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741854_1030{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:18,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0006/__spark_libs__4874354695191714472.zip is closed by DFSClient_NONMAPREDUCE_1405305727_1
2020-08-31 06:45:18,716 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0006/__spark_conf__.zip
2020-08-31 06:45:18,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:18,738 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741855_1031{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:45:18,742 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741855_1031 size 183859
2020-08-31 06:45:18,743 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0006/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1405305727_1
2020-08-31 06:45:30,217 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741855_1031 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:45:30,217 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741851_1027 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:45:30,217 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741852_1028 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:45:30,217 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741853_1029 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 06:45:30,217 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741854_1030 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:45:30,279 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741851_1027, blk_1073741852_1028, blk_1073741853_1029, blk_1073741854_1030, blk_1073741855_1031]
2020-08-31 06:45:33,280 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741851_1027, blk_1073741852_1028, blk_1073741853_1029, blk_1073741854_1030, blk_1073741855_1031]
2020-08-31 06:45:36,280 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741851_1027, blk_1073741852_1028, blk_1073741853_1029, blk_1073741854_1030, blk_1073741855_1031]
2020-08-31 06:46:05,785 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0007/__spark_libs__1813373432255985260.zip
2020-08-31 06:46:06,399 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:46:06,399 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741856_1032{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:46:06,404 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0007/__spark_libs__1813373432255985260.zip
2020-08-31 06:46:06,406 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741856_1032 size 67108864
2020-08-31 06:46:06,670 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741857_1033{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:46:06,678 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0007/__spark_libs__1813373432255985260.zip
2020-08-31 06:46:06,679 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741857_1033 size 67108864
2020-08-31 06:46:06,679 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741857_1033 size 67108864
2020-08-31 06:46:06,985 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:06,985 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:06,986 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741858_1034{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:06,988 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0007/__spark_libs__1813373432255985260.zip
2020-08-31 06:46:07,133 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:07,134 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:07,135 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741859_1035{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:07,140 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0007/__spark_libs__1813373432255985260.zip is closed by DFSClient_NONMAPREDUCE_-166709537_1
2020-08-31 06:46:07,457 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0007/__spark_conf__.zip
2020-08-31 06:46:07,479 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:46:07,479 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:46:07,482 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741860_1036{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:46:07,485 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0007/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-166709537_1
2020-08-31 06:46:19,219 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 186 Total time for transactions(ms): 34 Number of transactions batched in Syncs: 0 Number of syncs: 113 SyncTimes(ms): 969 
2020-08-31 06:46:19,221 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741860_1036 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:46:19,221 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741856_1032 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:46:19,221 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741857_1033 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:46:19,221 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741858_1034 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 06:46:19,221 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741859_1035 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 06:46:21,284 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035, blk_1073741860_1036]
2020-08-31 06:46:24,284 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035, blk_1073741860_1036]
2020-08-31 06:46:27,284 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741856_1032, blk_1073741857_1033, blk_1073741858_1034, blk_1073741859_1035, blk_1073741860_1036]
2020-08-31 06:46:36,962 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0008/__spark_libs__4393783461997153253.zip
2020-08-31 06:46:37,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:46:37,532 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741861_1037{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:46:37,538 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0008/__spark_libs__4393783461997153253.zip
2020-08-31 06:46:37,540 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741861_1037 size 67108864
2020-08-31 06:46:37,862 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:46:37,864 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741862_1038{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:46:37,878 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0008/__spark_libs__4393783461997153253.zip
2020-08-31 06:46:37,892 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741862_1038 size 67108864
2020-08-31 06:46:38,186 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:38,187 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741863_1039{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:38,192 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0008/__spark_libs__4393783461997153253.zip
2020-08-31 06:46:38,193 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741863_1039 size 67108864
2020-08-31 06:46:38,346 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:46:38,346 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:46:38,353 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741864_1040{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:46:38,358 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0008/__spark_libs__4393783461997153253.zip is closed by DFSClient_NONMAPREDUCE_-1160708189_1
2020-08-31 06:46:38,655 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0008/__spark_conf__.zip
2020-08-31 06:46:38,967 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:38,970 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741865_1041{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:46:38,975 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741865_1041 size 183859
2020-08-31 06:46:38,976 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0008/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1160708189_1
2020-08-31 06:46:51,576 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741865_1041 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:46:51,576 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741861_1037 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:46:51,576 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741862_1038 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 06:46:51,577 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741863_1039 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:46:51,577 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741864_1040 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:46:54,298 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741861_1037, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040, blk_1073741865_1041]
2020-08-31 06:46:57,298 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741861_1037, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040, blk_1073741865_1041]
2020-08-31 06:47:00,298 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741861_1037, blk_1073741862_1038, blk_1073741863_1039, blk_1073741864_1040, blk_1073741865_1041]
2020-08-31 06:48:07,274 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 213 Total time for transactions(ms): 35 Number of transactions batched in Syncs: 0 Number of syncs: 130 SyncTimes(ms): 999 
2020-08-31 06:48:10,030 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0009/__spark_libs__5191728711443465273.zip
2020-08-31 06:48:10,643 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:48:10,645 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741866_1042{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:48:10,649 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0009/__spark_libs__5191728711443465273.zip
2020-08-31 06:48:10,656 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741866_1042 size 67108864
2020-08-31 06:48:10,919 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:48:10,922 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:48:10,923 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741867_1043{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:48:10,924 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0009/__spark_libs__5191728711443465273.zip
2020-08-31 06:48:11,248 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:48:11,249 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:48:11,252 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741868_1044{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:48:11,253 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0009/__spark_libs__5191728711443465273.zip
2020-08-31 06:48:11,447 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:48:11,449 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741869_1045{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:48:11,455 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741869_1045 size 33364385
2020-08-31 06:48:11,456 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0009/__spark_libs__5191728711443465273.zip is closed by DFSClient_NONMAPREDUCE_-2070115050_1
2020-08-31 06:48:11,789 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0009/__spark_conf__.zip
2020-08-31 06:48:11,806 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:48:11,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741870_1046{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:48:11,815 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741870_1046 size 183859
2020-08-31 06:48:11,816 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0009/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-2070115050_1
2020-08-31 06:48:23,263 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741870_1046 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:48:23,263 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741866_1042 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 06:48:23,263 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741867_1043 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:48:23,263 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741868_1044 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:48:23,263 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741869_1045 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 06:48:24,305 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741866_1042, blk_1073741867_1043, blk_1073741868_1044, blk_1073741869_1045, blk_1073741870_1046]
2020-08-31 06:48:27,306 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741866_1042, blk_1073741867_1043, blk_1073741868_1044, blk_1073741869_1045, blk_1073741870_1046]
2020-08-31 06:48:30,306 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741866_1042, blk_1073741867_1043, blk_1073741868_1044, blk_1073741869_1045, blk_1073741870_1046]
2020-08-31 06:55:04,553 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 239 Total time for transactions(ms): 37 Number of transactions batched in Syncs: 0 Number of syncs: 146 SyncTimes(ms): 1024 
2020-08-31 06:55:07,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0010/__spark_libs__5353842207007150672.zip
2020-08-31 06:55:07,982 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:55:07,993 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:55:07,993 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741871_1047{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 06:55:08,005 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0010/__spark_libs__5353842207007150672.zip
2020-08-31 06:55:08,372 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:55:08,373 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:55:08,376 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741872_1048{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 06:55:08,378 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0010/__spark_libs__5353842207007150672.zip
2020-08-31 06:55:08,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:55:08,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:55:08,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741873_1049{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:55:08,703 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0010/__spark_libs__5353842207007150672.zip
2020-08-31 06:55:08,840 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741874_1050{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:55:08,848 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0010/__spark_libs__5353842207007150672.zip is closed by DFSClient_NONMAPREDUCE_870404952_1
2020-08-31 06:55:08,851 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741874_1050 size 33364385
2020-08-31 06:55:08,851 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741874_1050 size 33364385
2020-08-31 06:55:09,125 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0010/__spark_conf__.zip
2020-08-31 06:55:09,142 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:55:09,146 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:55:09,146 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741875_1051{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 06:55:09,150 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0010/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_870404952_1
2020-08-31 06:55:24,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741875_1051 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 06:55:24,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741871_1047 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 06:55:24,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741872_1048 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:55:24,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741873_1049 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 06:55:24,053 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741874_1050 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 06:55:24,346 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741872_1048, blk_1073741873_1049, blk_1073741874_1050, blk_1073741875_1051, blk_1073741871_1047]
2020-08-31 06:55:27,347 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741872_1048, blk_1073741873_1049, blk_1073741874_1050, blk_1073741875_1051, blk_1073741871_1047]
2020-08-31 06:55:30,347 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741872_1048, blk_1073741873_1049, blk_1073741874_1050, blk_1073741875_1051, blk_1073741871_1047]
2020-08-31 07:05:19,219 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 265 Total time for transactions(ms): 37 Number of transactions batched in Syncs: 0 Number of syncs: 162 SyncTimes(ms): 1067 
2020-08-31 07:05:22,026 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0011/__spark_libs__2664159889287330157.zip
2020-08-31 07:05:22,609 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:05:22,612 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:05:22,613 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741876_1052{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:05:22,615 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0011/__spark_libs__2664159889287330157.zip
2020-08-31 07:05:22,892 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:22,893 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:22,895 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741877_1053{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:22,897 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0011/__spark_libs__2664159889287330157.zip
2020-08-31 07:05:23,189 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:23,191 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:23,192 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741878_1054{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:23,196 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0011/__spark_libs__2664159889287330157.zip
2020-08-31 07:05:23,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:23,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:23,377 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741879_1055{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:05:23,382 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0011/__spark_libs__2664159889287330157.zip is closed by DFSClient_NONMAPREDUCE_-1183725881_1
2020-08-31 07:05:23,712 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0011/__spark_conf__.zip
2020-08-31 07:05:23,735 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:05:23,736 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741880_1056{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:05:23,740 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741880_1056 size 183859
2020-08-31 07:05:23,741 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0011/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1183725881_1
2020-08-31 07:05:40,104 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741880_1056 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 07:05:40,104 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741876_1052 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:05:40,104 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741877_1053 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:05:40,104 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741878_1054 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:05:40,104 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741879_1055 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:05:42,385 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741876_1052, blk_1073741877_1053, blk_1073741878_1054, blk_1073741879_1055, blk_1073741880_1056]
2020-08-31 07:05:45,385 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741876_1052, blk_1073741877_1053, blk_1073741878_1054, blk_1073741879_1055, blk_1073741880_1056]
2020-08-31 07:05:48,385 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741876_1052, blk_1073741877_1053, blk_1073741878_1054, blk_1073741879_1055, blk_1073741880_1056]
2020-08-31 07:07:19,127 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 291 Total time for transactions(ms): 38 Number of transactions batched in Syncs: 0 Number of syncs: 178 SyncTimes(ms): 1094 
2020-08-31 07:07:21,975 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0012/__spark_libs__8803923229571538325.zip
2020-08-31 07:07:22,585 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:07:22,586 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741881_1057{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:07:22,590 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0012/__spark_libs__8803923229571538325.zip
2020-08-31 07:07:22,592 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741881_1057 size 67108864
2020-08-31 07:07:23,070 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:07:23,071 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741882_1058{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:07:23,073 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0012/__spark_libs__8803923229571538325.zip
2020-08-31 07:07:23,073 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741882_1058 size 67108864
2020-08-31 07:07:23,432 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741883_1059{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:07:23,437 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0012/__spark_libs__8803923229571538325.zip
2020-08-31 07:07:23,440 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741883_1059 size 67108864
2020-08-31 07:07:23,440 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741883_1059 size 67108864
2020-08-31 07:07:23,609 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:07:23,611 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741884_1060{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:07:23,615 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741884_1060 size 33364385
2020-08-31 07:07:23,616 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0012/__spark_libs__8803923229571538325.zip is closed by DFSClient_NONMAPREDUCE_82969646_1
2020-08-31 07:07:23,986 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0012/__spark_conf__.zip
2020-08-31 07:07:24,014 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:07:24,016 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741885_1061{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:07:24,020 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741885_1061 size 183859
2020-08-31 07:07:24,020 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0012/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_82969646_1
2020-08-31 07:07:38,218 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741885_1061 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:07:38,218 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741881_1057 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 07:07:38,218 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741882_1058 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:07:38,218 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741883_1059 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:07:38,218 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741884_1060 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 07:07:39,410 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741881_1057, blk_1073741882_1058, blk_1073741883_1059, blk_1073741884_1060, blk_1073741885_1061]
2020-08-31 07:07:42,410 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741881_1057, blk_1073741882_1058, blk_1073741883_1059, blk_1073741884_1060, blk_1073741885_1061]
2020-08-31 07:07:45,410 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741881_1057, blk_1073741882_1058, blk_1073741883_1059, blk_1073741884_1060, blk_1073741885_1061]
2020-08-31 07:09:40,630 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-31 07:09:40,630 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-31 07:09:40,630 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 30
2020-08-31 07:09:40,630 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 317 Total time for transactions(ms): 41 Number of transactions batched in Syncs: 0 Number of syncs: 194 SyncTimes(ms): 1333 
2020-08-31 07:09:40,632 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 317 Total time for transactions(ms): 41 Number of transactions batched in Syncs: 0 Number of syncs: 195 SyncTimes(ms): 1335 
2020-08-31 07:09:40,633 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000030 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000030-0000000000000000346
2020-08-31 07:09:40,633 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 347
2020-08-31 07:09:40,762 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-31 07:09:40,762 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000346 size 671 bytes.
2020-08-31 07:09:40,765 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 29
2020-08-31 07:09:40,765 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000027, cpktTxId=0000000000000000027)
2020-08-31 07:13:45,029 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 1 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 16 
2020-08-31 07:13:47,882 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0013/__spark_libs__5414209858546584613.zip
2020-08-31 07:13:48,465 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:13:48,471 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:13:48,471 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741886_1062{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:13:48,479 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0013/__spark_libs__5414209858546584613.zip
2020-08-31 07:13:48,775 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:13:48,775 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:13:48,775 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741887_1063{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:13:48,777 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0013/__spark_libs__5414209858546584613.zip
2020-08-31 07:13:49,109 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:13:49,110 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:13:49,111 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741888_1064{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:13:49,112 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0013/__spark_libs__5414209858546584613.zip
2020-08-31 07:13:49,284 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:13:49,287 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:13:49,288 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741889_1065{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:13:49,292 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0013/__spark_libs__5414209858546584613.zip is closed by DFSClient_NONMAPREDUCE_52562099_1
2020-08-31 07:13:49,593 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0013/__spark_conf__.zip
2020-08-31 07:13:49,609 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:13:49,610 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:13:49,612 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741890_1066{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:13:49,614 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0013/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_52562099_1
2020-08-31 07:14:06,124 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741890_1066 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 07:14:06,124 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741886_1062 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:14:06,124 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741887_1063 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:14:06,124 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741888_1064 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 07:14:06,124 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741889_1065 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:14:06,438 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741888_1064, blk_1073741889_1065, blk_1073741890_1066, blk_1073741886_1062, blk_1073741887_1063]
2020-08-31 07:14:09,439 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741888_1064, blk_1073741889_1065, blk_1073741890_1066, blk_1073741886_1062, blk_1073741887_1063]
2020-08-31 07:14:12,439 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741888_1064, blk_1073741889_1065, blk_1073741890_1066, blk_1073741886_1062, blk_1073741887_1063]
2020-08-31 07:16:50,404 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 28 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 37 
2020-08-31 07:16:53,195 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0014/__spark_libs__6395551665683394249.zip
2020-08-31 07:16:53,803 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:16:53,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:16:53,808 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741891_1067{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:16:53,813 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0014/__spark_libs__6395551665683394249.zip
2020-08-31 07:16:54,136 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:54,136 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:54,137 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741892_1068{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:54,141 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0014/__spark_libs__6395551665683394249.zip
2020-08-31 07:16:54,464 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:54,468 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:54,469 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741893_1069{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:54,470 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0014/__spark_libs__6395551665683394249.zip
2020-08-31 07:16:54,627 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:16:54,628 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:16:54,630 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741894_1070{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:16:54,638 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0014/__spark_libs__6395551665683394249.zip is closed by DFSClient_NONMAPREDUCE_507353177_1
2020-08-31 07:16:54,978 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0014/__spark_conf__.zip
2020-08-31 07:16:54,995 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:54,997 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741895_1071{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:16:55,000 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741895_1071 size 183859
2020-08-31 07:16:55,002 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0014/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_507353177_1
2020-08-31 07:17:13,245 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741895_1071 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:17:13,245 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741891_1067 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:17:13,245 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741892_1068 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:17:13,245 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741893_1069 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:17:13,245 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741894_1070 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:17:15,451 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741891_1067, blk_1073741892_1068, blk_1073741893_1069, blk_1073741894_1070, blk_1073741895_1071]
2020-08-31 07:17:18,451 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741891_1067, blk_1073741892_1068, blk_1073741893_1069, blk_1073741894_1070, blk_1073741895_1071]
2020-08-31 07:17:21,452 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741891_1067, blk_1073741892_1068, blk_1073741893_1069, blk_1073741894_1070, blk_1073741895_1071]
2020-08-31 07:19:05,653 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 54 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 34 SyncTimes(ms): 69 
2020-08-31 07:19:08,399 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0015/__spark_libs__686096513903755349.zip
2020-08-31 07:19:08,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:19:08,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:19:08,974 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741896_1072{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:19:08,977 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0015/__spark_libs__686096513903755349.zip
2020-08-31 07:19:09,248 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741897_1073{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:19:09,252 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0015/__spark_libs__686096513903755349.zip
2020-08-31 07:19:09,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741897_1073 size 67108864
2020-08-31 07:19:09,255 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741897_1073 size 67108864
2020-08-31 07:19:09,578 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:19:09,578 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:19:09,581 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741898_1074{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:19:09,582 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0015/__spark_libs__686096513903755349.zip
2020-08-31 07:19:09,771 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:19:09,772 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741899_1075{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:19:09,778 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741899_1075 size 33364385
2020-08-31 07:19:09,779 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0015/__spark_libs__686096513903755349.zip is closed by DFSClient_NONMAPREDUCE_1175790329_1
2020-08-31 07:19:10,085 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0015/__spark_conf__.zip
2020-08-31 07:19:10,101 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:19:10,103 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:19:10,104 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741900_1076{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:19:10,107 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0015/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1175790329_1
2020-08-31 07:19:24,954 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741900_1076 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:19:24,955 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741896_1072 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:19:24,955 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741897_1073 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:19:24,955 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741898_1074 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:19:24,955 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741899_1075 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 07:19:27,468 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741896_1072, blk_1073741897_1073, blk_1073741898_1074, blk_1073741899_1075, blk_1073741900_1076]
2020-08-31 07:19:30,468 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741896_1072, blk_1073741897_1073, blk_1073741898_1074, blk_1073741899_1075, blk_1073741900_1076]
2020-08-31 07:19:33,469 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741896_1072, blk_1073741897_1073, blk_1073741898_1074, blk_1073741899_1075, blk_1073741900_1076]
2020-08-31 07:20:14,510 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 80 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 50 SyncTimes(ms): 92 
2020-08-31 07:20:17,251 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0016/__spark_libs__7856753890149669771.zip
2020-08-31 07:20:17,833 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:20:17,834 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:20:17,834 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741901_1077{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:20:17,842 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0016/__spark_libs__7856753890149669771.zip
2020-08-31 07:20:18,159 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:18,160 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:18,161 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741902_1078{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:18,165 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0016/__spark_libs__7856753890149669771.zip
2020-08-31 07:20:18,503 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:20:18,503 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:20:18,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741903_1079{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:20:18,506 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0016/__spark_libs__7856753890149669771.zip
2020-08-31 07:20:18,652 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:18,653 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741904_1080{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:18,659 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741904_1080 size 33364385
2020-08-31 07:20:18,660 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0016/__spark_libs__7856753890149669771.zip is closed by DFSClient_NONMAPREDUCE_1592177950_1
2020-08-31 07:20:19,036 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0016/__spark_conf__.zip
2020-08-31 07:20:19,055 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:19,056 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:19,058 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741905_1081{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:20:19,060 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0016/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1592177950_1
2020-08-31 07:20:34,346 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741905_1081 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:20:34,346 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741901_1077 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:20:34,346 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741902_1078 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:20:34,346 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741903_1079 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:20:34,347 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741904_1080 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:20:36,491 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741904_1080, blk_1073741905_1081, blk_1073741901_1077, blk_1073741902_1078, blk_1073741903_1079]
2020-08-31 07:20:39,492 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741904_1080, blk_1073741905_1081, blk_1073741901_1077, blk_1073741902_1078, blk_1073741903_1079]
2020-08-31 07:20:42,492 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741904_1080, blk_1073741905_1081, blk_1073741901_1077, blk_1073741902_1078, blk_1073741903_1079]
2020-08-31 07:21:12,832 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0017/__spark_libs__6374511704851483641.zip
2020-08-31 07:21:13,457 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:13,461 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:13,461 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741906_1082{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:13,464 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0017/__spark_libs__6374511704851483641.zip
2020-08-31 07:21:13,737 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:21:13,738 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:21:13,739 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741907_1083{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:21:13,744 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741908_1084{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0017/__spark_libs__6374511704851483641.zip
2020-08-31 07:21:14,093 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0017/__spark_libs__6374511704851483641.zip
2020-08-31 07:21:14,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741908_1084{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 67108864
2020-08-31 07:21:14,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741908_1084 size 67108864
2020-08-31 07:21:14,094 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741908_1084 size 67108864
2020-08-31 07:21:14,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:14,273 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:14,276 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741909_1085{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:14,280 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0017/__spark_libs__6374511704851483641.zip is closed by DFSClient_NONMAPREDUCE_-236081640_1
2020-08-31 07:21:14,768 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 124 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 76 SyncTimes(ms): 133 
2020-08-31 07:21:14,775 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0017/__spark_conf__.zip
2020-08-31 07:21:14,796 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:14,802 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:14,802 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741910_1086{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:21:14,804 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0017/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-236081640_1
2020-08-31 07:21:30,777 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741910_1086 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:21:30,778 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741906_1082 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:21:30,778 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741907_1083 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:21:30,778 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741908_1084 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:21:30,778 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741909_1085 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:21:33,501 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741906_1082, blk_1073741907_1083, blk_1073741908_1084, blk_1073741909_1085, blk_1073741910_1086]
2020-08-31 07:21:36,501 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741906_1082, blk_1073741907_1083, blk_1073741908_1084, blk_1073741909_1085, blk_1073741910_1086]
2020-08-31 07:21:39,502 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741906_1082, blk_1073741907_1083, blk_1073741908_1084, blk_1073741909_1085, blk_1073741910_1086]
2020-08-31 07:23:07,795 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 132 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 82 SyncTimes(ms): 144 
2020-08-31 07:23:10,595 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0018/__spark_libs__7065297581321216503.zip
2020-08-31 07:23:11,197 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:23:11,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:23:11,204 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741911_1087{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:23:11,205 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0018/__spark_libs__7065297581321216503.zip
2020-08-31 07:23:11,511 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741912_1088{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:23:11,515 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0018/__spark_libs__7065297581321216503.zip
2020-08-31 07:23:11,522 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741912_1088 size 67108864
2020-08-31 07:23:11,522 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741912_1088 size 67108864
2020-08-31 07:23:11,834 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:23:11,834 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741913_1089{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:23:11,838 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0018/__spark_libs__7065297581321216503.zip
2020-08-31 07:23:11,838 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741913_1089 size 67108864
2020-08-31 07:23:11,987 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:23:11,989 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741914_1090{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:23:11,995 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741914_1090 size 33364385
2020-08-31 07:23:11,997 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0018/__spark_libs__7065297581321216503.zip is closed by DFSClient_NONMAPREDUCE_-1669463920_1
2020-08-31 07:23:12,341 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0018/__spark_conf__.zip
2020-08-31 07:23:12,357 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:23:12,358 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741915_1091{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:23:12,361 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741915_1091 size 183859
2020-08-31 07:23:12,362 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0018/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-1669463920_1
2020-08-31 07:23:17,085 INFO BlockStateChange: BLOCK* processReport 0x753cdb4b79d: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 6, hasStaleStorage: false, processing time: 0 msecs
2020-08-31 07:23:28,663 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741915_1091 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:23:28,663 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741911_1087 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:23:28,663 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741912_1088 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:23:28,663 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741913_1089 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:23:28,663 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741914_1090 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:23:30,521 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741911_1087, blk_1073741912_1088, blk_1073741913_1089, blk_1073741914_1090, blk_1073741915_1091]
2020-08-31 07:23:33,522 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741911_1087, blk_1073741912_1088, blk_1073741913_1089, blk_1073741914_1090, blk_1073741915_1091]
2020-08-31 07:23:36,522 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741911_1087, blk_1073741912_1088, blk_1073741913_1089, blk_1073741914_1090, blk_1073741915_1091]
2020-08-31 07:25:45,954 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 158 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 98 SyncTimes(ms): 177 
2020-08-31 07:25:48,731 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0019/__spark_libs__7815687798669266144.zip
2020-08-31 07:25:49,330 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:25:49,332 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:25:49,344 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741916_1092{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:25:49,349 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0019/__spark_libs__7815687798669266144.zip
2020-08-31 07:25:49,660 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:25:49,660 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:25:49,660 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741917_1093{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:25:49,666 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0019/__spark_libs__7815687798669266144.zip
2020-08-31 07:25:49,991 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:25:49,991 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741918_1094{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:25:49,992 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0019/__spark_libs__7815687798669266144.zip
2020-08-31 07:25:49,994 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741918_1094 size 67108864
2020-08-31 07:25:50,325 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:25:50,327 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:25:50,327 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741919_1095{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:25:50,333 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0019/__spark_libs__7815687798669266144.zip is closed by DFSClient_NONMAPREDUCE_-56267670_1
2020-08-31 07:25:50,629 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0019/__spark_conf__.zip
2020-08-31 07:25:50,644 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:25:50,645 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741920_1096{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:25:50,647 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741920_1096 size 183859
2020-08-31 07:25:50,649 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0019/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-56267670_1
2020-08-31 07:28:22,351 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 183 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 113 SyncTimes(ms): 395 
2020-08-31 07:28:22,353 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741920_1096 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:28:22,353 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741916_1092 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:28:22,353 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741917_1093 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:28:22,353 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741918_1094 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:28:22,353 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741919_1095 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:28:24,555 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741920_1096, blk_1073741916_1092, blk_1073741917_1093, blk_1073741918_1094, blk_1073741919_1095]
2020-08-31 07:28:27,555 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741920_1096, blk_1073741916_1092, blk_1073741917_1093, blk_1073741918_1094, blk_1073741919_1095]
2020-08-31 07:28:30,555 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741920_1096, blk_1073741916_1092, blk_1073741917_1093, blk_1073741918_1094, blk_1073741919_1095]
2020-08-31 07:30:43,555 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 184 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 0 Number of syncs: 114 SyncTimes(ms): 397 
2020-08-31 07:30:46,325 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0020/__spark_libs__1160816790653747330.zip
2020-08-31 07:30:46,922 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:46,922 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741921_1097{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:46,923 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0020/__spark_libs__1160816790653747330.zip
2020-08-31 07:30:46,924 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741921_1097 size 67108864
2020-08-31 07:30:47,201 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:47,203 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741922_1098{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:47,206 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0020/__spark_libs__1160816790653747330.zip
2020-08-31 07:30:47,212 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741922_1098 size 67108864
2020-08-31 07:30:47,558 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:47,560 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:47,560 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741923_1099{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:47,563 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0020/__spark_libs__1160816790653747330.zip
2020-08-31 07:30:47,752 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:30:47,752 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:30:47,756 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741924_1100{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:30:47,758 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0020/__spark_libs__1160816790653747330.zip is closed by DFSClient_NONMAPREDUCE_-488008857_1
2020-08-31 07:30:48,111 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598851313344_0020/__spark_conf__.zip
2020-08-31 07:30:48,129 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:48,134 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741925_1101{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:30:48,134 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741925_1101 size 183859
2020-08-31 07:30:48,135 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598851313344_0020/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-488008857_1
2020-08-31 07:32:29,080 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 209 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 129 SyncTimes(ms): 419 
2020-08-31 07:32:29,083 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741925_1101 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:32:29,083 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741921_1097 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:32:29,083 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741922_1098 10.178.0.23:50010 10.178.0.22:50010 10.178.0.24:50010 
2020-08-31 07:32:29,083 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741923_1099 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:32:29,083 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741924_1100 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:32:30,591 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.24:50010 to delete [blk_1073741921_1097, blk_1073741922_1098, blk_1073741923_1099, blk_1073741924_1100, blk_1073741925_1101]
2020-08-31 07:32:33,592 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741921_1097, blk_1073741922_1098, blk_1073741923_1099, blk_1073741924_1100, blk_1073741925_1101]
2020-08-31 07:32:36,592 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741921_1097, blk_1073741922_1098, blk_1073741923_1099, blk_1073741924_1100, blk_1073741925_1101]
2020-08-31 07:44:56,774 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-31 07:44:56,776 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-31 07:47:55,243 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/spark-2.3.1-yarn-shuffle.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-31 07:47:55,255 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-31 07:47:55,261 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-31 07:47:55,664 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-31 07:47:55,790 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-31 07:47:55,790 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-31 07:47:55,793 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-31 07:47:55,793 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-31 07:47:56,093 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-31 07:47:56,154 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-31 07:47:56,164 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-31 07:47:56,186 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-31 07:47:56,193 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-31 07:47:56,196 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-31 07:47:56,197 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-31 07:47:56,197 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-31 07:47:56,357 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-31 07:47:56,358 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-31 07:47:56,375 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-31 07:47:56,375 INFO org.mortbay.log: jetty-6.1.26
2020-08-31 07:47:56,535 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-31 07:47:56,576 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 07:47:56,576 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 07:47:56,611 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-31 07:47:56,612 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-31 07:47:56,614 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-31 07:47:56,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-31 07:47:56,657 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-31 07:47:56,658 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-31 07:47:56,659 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 31 07:47:56
2020-08-31 07:47:56,661 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-31 07:47:56,661 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:47:56,667 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-31 07:47:56,667 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-31 07:47:56,675 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-31 07:47:56,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-31 07:47:56,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-31 07:47:56,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-31 07:47:56,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-31 07:47:56,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-31 07:47:56,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-31 07:47:56,676 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-31 07:47:56,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-31 07:47:56,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-31 07:47:56,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-31 07:47:56,685 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-31 07:47:56,687 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-31 07:47:56,773 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-31 07:47:56,773 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:47:56,773 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-31 07:47:56,773 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-31 07:47:56,774 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-31 07:47:56,774 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-31 07:47:56,774 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-31 07:47:56,774 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-31 07:47:56,784 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-31 07:47:56,784 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:47:56,784 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-31 07:47:56,788 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-31 07:47:56,790 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-31 07:47:56,790 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-31 07:47:56,791 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-31 07:47:56,794 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-31 07:47:56,794 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-31 07:47:56,794 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-31 07:47:56,800 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-31 07:47:56,800 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-31 07:47:56,803 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-31 07:47:56,803 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:47:56,803 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-31 07:47:56,803 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-31 07:47:56,819 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 9366@producer.c.poetic-set-285601.internal
2020-08-31 07:47:56,893 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-31 07:47:57,078 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000347 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000347-0000000000000000555
2020-08-31 07:47:57,088 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000346, cpktTxId=0000000000000000346)
2020-08-31 07:47:57,118 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 6 INodes.
2020-08-31 07:47:57,146 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-31 07:47:57,147 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 346 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000346
2020-08-31 07:47:57,147 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@758a34ce expecting start txid #347
2020-08-31 07:47:57,147 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000347-0000000000000000555
2020-08-31 07:47:57,149 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000347-0000000000000000555' to transaction ID 347
2020-08-31 07:47:57,198 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000347-0000000000000000555 of size 1048576 edits # 209 loaded in 0 seconds
2020-08-31 07:47:57,199 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-31 07:47:57,204 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 5 milliseconds
name space=6
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-31 07:47:57,204 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-31 07:47:57,207 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 556
2020-08-31 07:47:57,347 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-31 07:47:57,347 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 540 msecs
2020-08-31 07:47:57,531 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-31 07:47:57,544 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-31 07:47:57,560 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-31 07:47:57,641 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-31 07:47:57,653 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 07:47:57,653 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 07:47:57,653 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-31 07:47:57,653 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-08-31 07:47:57,653 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-31 07:47:57,653 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-31 07:47:57,662 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:47:57,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-31 07:47:57,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-31 07:47:57,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-31 07:47:57,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-31 07:47:57,720 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-31 07:47:57,720 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 66 msec
2020-08-31 07:47:57,725 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-31 07:47:57,726 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-31 07:47:57,730 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-31 07:47:57,730 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-31 07:47:57,734 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-31 07:48:02,420 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-31 07:48:02,420 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:48:02,422 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-31 07:48:02,510 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:48:02,510 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-31 07:48:02,559 INFO BlockStateChange: BLOCK* processReport 0x8ad9ed80254: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 2 msecs
2020-08-31 07:48:02,631 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-31 07:48:02,631 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:48:02,631 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-31 07:48:02,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:48:02,674 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-31 07:48:02,708 INFO BlockStateChange: BLOCK* processReport 0x8adb325138e: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 1 msecs
2020-08-31 07:48:03,436 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-31 07:48:03,437 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:48:03,437 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-31 07:48:03,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:48:03,476 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-31 07:48:03,509 INFO BlockStateChange: BLOCK* processReport 0x8ae14cd42ce: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 1, hasStaleStorage: false, processing time: 2 msecs
2020-08-31 07:48:31,200 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741926_1102{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860093893_0001/__spark_libs__553234405253257408.zip
2020-08-31 07:48:33,075 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860093893_0001/__spark_libs__553234405253257408.zip
2020-08-31 07:48:33,077 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741926_1102{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 67108864
2020-08-31 07:48:33,078 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741926_1102 size 67108864
2020-08-31 07:48:33,090 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741926_1102 size 67108864
2020-08-31 07:48:34,020 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:48:34,021 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741927_1103{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:48:34,034 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860093893_0001/__spark_libs__553234405253257408.zip
2020-08-31 07:48:34,043 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741927_1103 size 67108864
2020-08-31 07:48:34,871 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:48:34,874 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:48:34,876 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741928_1104{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 07:48:34,878 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860093893_0001/__spark_libs__553234405253257408.zip
2020-08-31 07:48:35,309 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:48:35,311 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741929_1105{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:48:35,329 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741929_1105 size 33364385
2020-08-31 07:48:35,330 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598860093893_0001/__spark_libs__553234405253257408.zip is closed by DFSClient_NONMAPREDUCE_1728550360_1
2020-08-31 07:48:35,706 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860093893_0001/__spark_conf__.zip
2020-08-31 07:48:35,747 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741930_1106{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:48:35,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741930_1106 size 183858
2020-08-31 07:48:35,759 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741930_1106 size 183858
2020-08-31 07:48:35,759 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598860093893_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_1728550360_1
2020-08-31 07:49:07,552 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-31 07:49:07,552 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-31 07:49:07,552 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 556
2020-08-31 07:49:07,552 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 27 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 17 SyncTimes(ms): 122 
2020-08-31 07:49:07,554 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 27 Total time for transactions(ms): 33 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 123 
2020-08-31 07:49:07,555 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000556 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000556-0000000000000000582
2020-08-31 07:49:07,556 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 583
2020-08-31 07:49:08,848 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-31 07:49:08,848 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000582 size 993 bytes.
2020-08-31 07:49:08,854 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 346
2020-08-31 07:49:08,854 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000029, cpktTxId=0000000000000000029)
2020-08-31 07:49:30,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741930_1106 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:49:30,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741926_1102 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:49:30,399 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741927_1103 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:49:30,400 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741928_1104 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 07:49:30,400 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741929_1105 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:49:30,794 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741926_1102, blk_1073741927_1103, blk_1073741928_1104, blk_1073741929_1105, blk_1073741930_1106]
2020-08-31 07:49:32,051 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-31 07:49:32,053 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-31 07:52:42,350 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/spark-2.3.1-yarn-shuffle.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-31 07:52:42,362 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-31 07:52:42,370 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-31 07:52:42,783 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-31 07:52:42,929 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-31 07:52:42,929 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-31 07:52:42,932 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-31 07:52:42,933 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-31 07:52:43,187 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-31 07:52:43,245 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-31 07:52:43,255 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-31 07:52:43,276 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-31 07:52:43,283 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-31 07:52:43,286 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-31 07:52:43,286 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-31 07:52:43,286 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-31 07:52:43,438 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-31 07:52:43,439 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-31 07:52:43,455 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-31 07:52:43,455 INFO org.mortbay.log: jetty-6.1.26
2020-08-31 07:52:43,611 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-31 07:52:43,655 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 07:52:43,655 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 07:52:43,689 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-31 07:52:43,690 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-31 07:52:43,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-31 07:52:43,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-31 07:52:43,730 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-31 07:52:43,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-31 07:52:43,732 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 31 07:52:43
2020-08-31 07:52:43,734 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-31 07:52:43,734 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:52:43,736 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-31 07:52:43,736 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-31 07:52:43,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-31 07:52:43,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-31 07:52:43,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-31 07:52:43,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-31 07:52:43,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-31 07:52:43,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-31 07:52:43,744 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-31 07:52:43,745 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-31 07:52:43,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-31 07:52:43,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-31 07:52:43,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-31 07:52:43,753 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-31 07:52:43,755 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-31 07:52:43,819 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-31 07:52:43,819 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:52:43,819 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-31 07:52:43,819 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-31 07:52:43,820 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-31 07:52:43,820 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-31 07:52:43,820 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-31 07:52:43,820 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-31 07:52:43,829 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-31 07:52:43,829 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:52:43,830 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-31 07:52:43,830 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-31 07:52:43,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-31 07:52:43,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-31 07:52:43,834 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-31 07:52:43,840 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-31 07:52:43,840 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-31 07:52:43,840 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-31 07:52:43,845 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-31 07:52:43,845 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-31 07:52:43,849 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-31 07:52:43,849 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 07:52:43,849 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-31 07:52:43,849 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-31 07:52:43,865 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 11000@producer.c.poetic-set-285601.internal
2020-08-31 07:52:43,939 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-31 07:52:44,103 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000583 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000583-0000000000000000584
2020-08-31 07:52:44,115 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000582, cpktTxId=0000000000000000582)
2020-08-31 07:52:44,156 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 9 INodes.
2020-08-31 07:52:44,206 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-31 07:52:44,207 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 582 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000582
2020-08-31 07:52:44,207 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6b7906b3 expecting start txid #583
2020-08-31 07:52:44,207 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000583-0000000000000000584
2020-08-31 07:52:44,209 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000583-0000000000000000584' to transaction ID 583
2020-08-31 07:52:44,224 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000583-0000000000000000584 of size 1048576 edits # 2 loaded in 0 seconds
2020-08-31 07:52:44,224 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-31 07:52:44,231 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 7 milliseconds
name space=6
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-31 07:52:44,231 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-08-31 07:52:44,236 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 585
2020-08-31 07:52:44,370 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-31 07:52:44,370 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 517 msecs
2020-08-31 07:52:44,550 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-31 07:52:44,558 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-31 07:52:44,579 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-31 07:52:44,658 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-31 07:52:44,670 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 07:52:44,670 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 07:52:44,670 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-31 07:52:44,671 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-08-31 07:52:44,671 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-31 07:52:44,671 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-31 07:52:44,677 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:52:44,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-31 07:52:44,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-31 07:52:44,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-31 07:52:44,719 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-31 07:52:44,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-31 07:52:44,719 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-31 07:52:44,719 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 47 msec
2020-08-31 07:52:44,721 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-31 07:52:44,723 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-31 07:52:44,723 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-31 07:52:44,728 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-31 07:52:49,161 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-31 07:52:49,161 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:52:49,162 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-31 07:52:49,227 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-31 07:52:49,227 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:52:49,227 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-31 07:52:49,259 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:52:49,260 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-31 07:52:49,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:52:49,272 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-31 07:52:49,320 INFO BlockStateChange: BLOCK* processReport 0x8f06d20fbe5: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 6, hasStaleStorage: false, processing time: 3 msecs
2020-08-31 07:52:49,320 INFO BlockStateChange: BLOCK* processReport 0x8f06339e9b2: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 6, hasStaleStorage: false, processing time: 0 msecs
2020-08-31 07:52:49,648 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-31 07:52:49,649 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:52:49,649 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-31 07:52:49,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 07:52:49,689 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-31 07:52:49,709 INFO BlockStateChange: BLOCK* processReport 0x8f0b85f56bc: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 6, hasStaleStorage: false, processing time: 1 msecs
2020-08-31 07:53:18,770 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_libs__4726357777387527109.zip
2020-08-31 07:53:20,442 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:53:20,443 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741931_1107{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:53:20,444 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_libs__4726357777387527109.zip
2020-08-31 07:53:20,455 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741931_1107 size 67108864
2020-08-31 07:53:21,429 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:53:21,430 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741932_1108{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:53:21,434 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_libs__4726357777387527109.zip
2020-08-31 07:53:21,443 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741932_1108 size 67108864
2020-08-31 07:53:22,423 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:53:22,424 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741933_1109{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 07:53:22,429 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741934_1110{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_libs__4726357777387527109.zip
2020-08-31 07:53:22,431 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741933_1109 size 67108864
2020-08-31 07:53:22,692 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741934_1110{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_libs__4726357777387527109.zip
2020-08-31 07:53:22,693 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-08-31 07:53:22,699 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741934_1110{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 33364385
2020-08-31 07:53:22,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741934_1110 size 33364385
2020-08-31 07:53:22,700 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741934_1110 size 33364385
2020-08-31 07:53:23,103 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_libs__4726357777387527109.zip is closed by DFSClient_NONMAPREDUCE_108010968_1
2020-08-31 07:53:23,562 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_conf__.zip
2020-08-31 07:53:23,590 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:53:23,591 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:53:23,591 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741935_1111{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW]]} size 0
2020-08-31 07:53:23,611 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598860383699_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_108010968_1
2020-08-31 07:53:54,465 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-31 07:53:54,465 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-31 07:53:54,465 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 585
2020-08-31 07:53:54,465 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 27 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 18 SyncTimes(ms): 122 
2020-08-31 07:53:54,467 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 27 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 19 SyncTimes(ms): 124 
2020-08-31 07:53:54,469 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000585 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000585-0000000000000000611
2020-08-31 07:53:54,469 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 612
2020-08-31 07:53:55,679 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-31 07:53:55,680 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000611 size 994 bytes.
2020-08-31 07:53:55,696 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 582
2020-08-31 07:53:55,696 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000346, cpktTxId=0000000000000000346)
2020-08-31 07:56:21,867 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 9 
2020-08-31 07:56:21,869 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741935_1111 10.178.0.24:50010 10.178.0.22:50010 10.178.0.23:50010 
2020-08-31 07:56:21,870 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741931_1107 10.178.0.23:50010 10.178.0.24:50010 10.178.0.22:50010 
2020-08-31 07:56:21,870 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741932_1108 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:56:21,870 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741933_1109 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 07:56:21,870 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741934_1110 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 07:56:23,802 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.22:50010 to delete [blk_1073741931_1107, blk_1073741932_1108, blk_1073741933_1109, blk_1073741934_1110, blk_1073741935_1111]
2020-08-31 07:56:24,210 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-31 07:56:24,212 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
2020-08-31 09:01:02,258 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = producer.c.poetic-set-285601.internal/10.178.0.25
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.7
STARTUP_MSG:   classpath = /home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/etc/hadoop:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-configuration-1.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-httpclient-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpclient-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/mockito-all-1.8.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-sslengine-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-digester-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsp-api-2.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsch-0.1.54.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-framework-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-net-3.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/gson-2.2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jets3t-0.9.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/commons-math3-3.1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/httpcore-4.2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/lib/curator-client-2.7.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/hadoop-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/common/spark-2.3.1-yarn-shuffle.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/activation-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-json-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-cli-1.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/servlet-api-2.5.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guava-11.0.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-codec-1.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jetty-6.1.26.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-client-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jettison-1.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/commons-lang-2.6.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-client-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-registry-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/xz-1.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/junit-4.11.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/guice-3.0.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/asm-3.2.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/lib/javax.inject-1.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.7-tests.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.7.jar:/home/jinhuijun/streaming-benchmarks/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.7.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar:/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = Unknown -r c1aad84bd27cd79c3d1a7dd58202a8c3ee1ed3ac; compiled by 'stevel' on 2018-07-18T22:47Z
STARTUP_MSG:   java = 1.8.0_262
************************************************************/
2020-08-31 09:01:02,269 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-08-31 09:01:02,275 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-08-31 09:01:02,682 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-08-31 09:01:02,827 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-08-31 09:01:02,827 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-08-31 09:01:02,830 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://10.178.0.25:9007
2020-08-31 09:01:02,831 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use 10.178.0.25:9007 to access this namenode/service.
2020-08-31 09:01:03,054 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://0.0.0.0:50070
2020-08-31 09:01:03,118 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-08-31 09:01:03,128 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-08-31 09:01:03,151 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-08-31 09:01:03,158 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-08-31 09:01:03,161 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-08-31 09:01:03,161 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-08-31 09:01:03,161 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-08-31 09:01:03,315 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-08-31 09:01:03,316 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-08-31 09:01:03,333 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-08-31 09:01:03,333 INFO org.mortbay.log: jetty-6.1.26
2020-08-31 09:01:03,485 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@0.0.0.0:50070
2020-08-31 09:01:03,531 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 09:01:03,531 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-08-31 09:01:03,567 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-08-31 09:01:03,568 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair: true
2020-08-31 09:01:03,570 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Detailed lock hold time metrics enabled: false
2020-08-31 09:01:03,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-08-31 09:01:03,608 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=false
2020-08-31 09:01:03,610 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-08-31 09:01:03,610 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Aug 31 09:01:03
2020-08-31 09:01:03,612 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-08-31 09:01:03,612 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 09:01:03,614 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-08-31 09:01:03,614 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-08-31 09:01:03,622 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-08-31 09:01:03,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 3
2020-08-31 09:01:03,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-08-31 09:01:03,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-08-31 09:01:03,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-08-31 09:01:03,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-08-31 09:01:03,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-08-31 09:01:03,623 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-08-31 09:01:03,634 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = jinhuijun (auth:SIMPLE)
2020-08-31 09:01:03,634 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-08-31 09:01:03,634 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = true
2020-08-31 09:01:03,634 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-08-31 09:01:03,636 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-08-31 09:01:03,723 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-08-31 09:01:03,724 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 09:01:03,724 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-08-31 09:01:03,724 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-08-31 09:01:03,725 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-08-31 09:01:03,725 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-08-31 09:01:03,725 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-08-31 09:01:03,725 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-08-31 09:01:03,735 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-08-31 09:01:03,735 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 09:01:03,735 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-08-31 09:01:03,735 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-08-31 09:01:03,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-08-31 09:01:03,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-08-31 09:01:03,742 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-08-31 09:01:03,746 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-08-31 09:01:03,746 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-08-31 09:01:03,746 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-08-31 09:01:03,752 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-08-31 09:01:03,752 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-08-31 09:01:03,754 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-08-31 09:01:03,754 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-08-31 09:01:03,755 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-08-31 09:01:03,755 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-08-31 09:01:03,775 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /tmp/hadoop-jinhuijun/dfs/name/in_use.lock acquired by nodename 13543@producer.c.poetic-set-285601.internal
2020-08-31 09:01:03,850 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /tmp/hadoop-jinhuijun/dfs/name/current
2020-08-31 09:01:03,993 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000612 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000612-0000000000000000613
2020-08-31 09:01:04,007 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000611, cpktTxId=0000000000000000611)
2020-08-31 09:01:04,050 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 9 INodes.
2020-08-31 09:01:04,098 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-08-31 09:01:04,099 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 611 from /tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000611
2020-08-31 09:01:04,099 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6b7906b3 expecting start txid #612
2020-08-31 09:01:04,099 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000612-0000000000000000613
2020-08-31 09:01:04,101 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000612-0000000000000000613' to transaction ID 612
2020-08-31 09:01:04,115 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000612-0000000000000000613 of size 1048576 edits # 2 loaded in 0 seconds
2020-08-31 09:01:04,115 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Initializing quota with 4 thread(s)
2020-08-31 09:01:04,122 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Quota initialization completed in 7 milliseconds
name space=6
storage space=45
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0
2020-08-31 09:01:04,122 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? true (staleImage=true, haEnabled=false, isRollingUpgrade=false)
2020-08-31 09:01:04,122 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Save namespace ...
2020-08-31 09:01:04,127 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Saving image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000613 using no compression
2020-08-31 09:01:04,166 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Image file /tmp/hadoop-jinhuijun/dfs/name/current/fsimage.ckpt_0000000000000000613 of size 671 bytes saved in 0 seconds.
2020-08-31 09:01:04,193 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 611
2020-08-31 09:01:04,193 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000582, cpktTxId=0000000000000000582)
2020-08-31 09:01:04,209 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 614
2020-08-31 09:01:04,328 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-08-31 09:01:04,328 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 569 msecs
2020-08-31 09:01:04,518 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to producer.c.poetic-set-285601.internal:9007
2020-08-31 09:01:04,527 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000
2020-08-31 09:01:04,547 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9007
2020-08-31 09:01:04,610 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-08-31 09:01:04,619 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 09:01:04,619 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-08-31 09:01:04,619 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-08-31 09:01:04,620 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 1 secs
2020-08-31 09:01:04,620 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-08-31 09:01:04,620 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-08-31 09:01:04,628 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 09:01:04,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 1
2020-08-31 09:01:04,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-08-31 09:01:04,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 1
2020-08-31 09:01:04,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-08-31 09:01:04,647 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-08-31 09:01:04,648 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 27 msec
2020-08-31 09:01:04,660 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-08-31 09:01:04,661 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9007: starting
2020-08-31 09:01:04,667 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: producer.c.poetic-set-285601.internal/10.178.0.25:9007
2020-08-31 09:01:04,668 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-08-31 09:01:04,675 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-08-31 09:01:09,148 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 1b7473ed-2ab4-4f61-ada0-0fa4ff46649a
2020-08-31 09:01:09,149 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 09:01:09,150 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.23:50010
2020-08-31 09:01:09,169 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage cb772a9d-0398-4417-8d81-bc9ca4bb78e6
2020-08-31 09:01:09,169 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 09:01:09,169 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.22:50010
2020-08-31 09:01:09,176 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0) storage 0f540cb0-8ad1-4388-9a83-261b63190790
2020-08-31 09:01:09,177 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 09:01:09,177 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/10.178.0.24:50010
2020-08-31 09:01:09,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 09:01:09,242 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-b555dc6f-661c-4aff-af53-78484b949fbe for DN 10.178.0.22:50010
2020-08-31 09:01:09,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 09:01:09,247 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-9238b537-5efc-40b2-aea9-0451db07acf7 for DN 10.178.0.23:50010
2020-08-31 09:01:09,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-08-31 09:01:09,253 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac for DN 10.178.0.24:50010
2020-08-31 09:01:09,290 INFO BlockStateChange: BLOCK* processReport 0xcaafc7e663c: from storage DS-b555dc6f-661c-4aff-af53-78484b949fbe node DatanodeRegistration(10.178.0.22:50010, datanodeUuid=cb772a9d-0398-4417-8d81-bc9ca4bb78e6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 11, hasStaleStorage: false, processing time: 2 msecs
2020-08-31 09:01:09,291 INFO BlockStateChange: BLOCK* processReport 0xcab071c4442: from storage DS-9238b537-5efc-40b2-aea9-0451db07acf7 node DatanodeRegistration(10.178.0.23:50010, datanodeUuid=1b7473ed-2ab4-4f61-ada0-0fa4ff46649a, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 11, hasStaleStorage: false, processing time: 1 msecs
2020-08-31 09:01:09,303 INFO BlockStateChange: BLOCK* processReport 0xcab39aeb196: from storage DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac node DatanodeRegistration(10.178.0.24:50010, datanodeUuid=0f540cb0-8ad1-4388-9a83-261b63190790, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3396c795-c47e-4e5c-869a-248846c8bf0f;nsid=1279826955;c=0), blocks: 11, hasStaleStorage: false, processing time: 1 msecs
2020-08-31 09:02:14,313 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Roll Edit Log from 10.178.0.25
2020-08-31 09:02:14,313 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Rolling edit logs
2020-08-31 09:02:14,313 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Ending log segment 614
2020-08-31 09:02:14,314 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 10 
2020-08-31 09:02:14,315 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 11 
2020-08-31 09:02:14,316 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /tmp/hadoop-jinhuijun/dfs/name/current/edits_inprogress_0000000000000000614 -> /tmp/hadoop-jinhuijun/dfs/name/current/edits_0000000000000000614-0000000000000000615
2020-08-31 09:02:14,317 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 616
2020-08-31 09:02:15,324 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Transfer took 0.00s at 0.00 KB/s
2020-08-31 09:02:15,324 INFO org.apache.hadoop.hdfs.server.namenode.TransferFsImage: Downloaded file fsimage.ckpt_0000000000000000615 size 671 bytes.
2020-08-31 09:02:15,329 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Going to retain 2 images with txid >= 613
2020-08-31 09:02:15,329 INFO org.apache.hadoop.hdfs.server.namenode.NNStorageRetentionManager: Purging old image FSImageFile(file=/tmp/hadoop-jinhuijun/dfs/name/current/fsimage_0000000000000000611, cpktTxId=0000000000000000611)
2020-08-31 09:06:18,122 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 0 Number of syncs: 2 SyncTimes(ms): 10 
2020-08-31 09:06:20,957 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598864483558_0001/__spark_libs__7013980504387630538.zip
2020-08-31 09:06:21,853 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 09:06:21,857 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741936_1112{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 09:06:21,864 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598864483558_0001/__spark_libs__7013980504387630538.zip
2020-08-31 09:06:21,870 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741936_1112 size 67108864
2020-08-31 09:06:22,216 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 09:06:22,217 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 09:06:22,218 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741937_1113{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 09:06:22,225 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598864483558_0001/__spark_libs__7013980504387630538.zip
2020-08-31 09:06:22,665 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 09:06:22,668 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 09:06:22,669 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741938_1114{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 09:06:22,671 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598864483558_0001/__spark_libs__7013980504387630538.zip
2020-08-31 09:06:22,838 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 09:06:22,839 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 09:06:22,842 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741939_1115{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW]]} size 0
2020-08-31 09:06:22,850 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598864483558_0001/__spark_libs__7013980504387630538.zip is closed by DFSClient_NONMAPREDUCE_-686247682_1
2020-08-31 09:06:23,143 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} for /user/jinhuijun/.sparkStaging/application_1598864483558_0001/__spark_conf__.zip
2020-08-31 09:06:23,164 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.24:50010 is added to blk_1073741940_1116{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-b555dc6f-661c-4aff-af53-78484b949fbe:NORMAL:10.178.0.22:50010|RBW], ReplicaUC[[DISK]DS-9238b537-5efc-40b2-aea9-0451db07acf7:NORMAL:10.178.0.23:50010|RBW], ReplicaUC[[DISK]DS-8ee74d7a-42b8-4b6c-a0f8-2d6fcd8de8ac:NORMAL:10.178.0.24:50010|RBW]]} size 0
2020-08-31 09:06:23,171 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.23:50010 is added to blk_1073741940_1116 size 183859
2020-08-31 09:06:23,171 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 10.178.0.22:50010 is added to blk_1073741940_1116 size 183859
2020-08-31 09:06:23,172 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /user/jinhuijun/.sparkStaging/application_1598864483558_0001/__spark_conf__.zip is closed by DFSClient_NONMAPREDUCE_-686247682_1
2020-08-31 09:11:24,939 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 27 Total time for transactions(ms): 12 Number of transactions batched in Syncs: 0 Number of syncs: 17 SyncTimes(ms): 29 
2020-08-31 09:11:24,940 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741940_1116 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 09:11:24,941 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741936_1112 10.178.0.24:50010 10.178.0.23:50010 10.178.0.22:50010 
2020-08-31 09:11:24,941 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741937_1113 10.178.0.22:50010 10.178.0.23:50010 10.178.0.24:50010 
2020-08-31 09:11:24,941 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741938_1114 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 09:11:24,941 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741939_1115 10.178.0.22:50010 10.178.0.24:50010 10.178.0.23:50010 
2020-08-31 09:11:25,750 INFO BlockStateChange: BLOCK* BlockManager: ask 10.178.0.23:50010 to delete [blk_1073741936_1112, blk_1073741937_1113, blk_1073741938_1114, blk_1073741939_1115, blk_1073741940_1116]
2020-08-31 09:11:27,352 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: RECEIVED SIGNAL 15: SIGTERM
2020-08-31 09:11:27,445 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at producer.c.poetic-set-285601.internal/10.178.0.25
************************************************************/
